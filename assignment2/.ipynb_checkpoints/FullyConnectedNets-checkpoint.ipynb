{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch/Layer Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (49000, 3, 32, 32))\n",
      "('y_train: ', (49000,))\n",
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_test: ', (1000,))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "4 5 6\n",
      "(4, 5, 6)\n",
      "3\n",
      "Testing affine_forward function:\n",
      "difference:  9.769849468192957e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "print(np.shape(input_shape))\n",
    "print(*input_shape)\n",
    "print(input_shape)\n",
    "print(len(input_shape))\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "[[-2.17606361 -2.12184729  0.00900847  2.20337442 -0.99083714]\n",
      " [-1.85215259 -3.19211285 -1.74370267  0.0973037  -0.54036486]\n",
      " [-1.56514011 -2.63372545 -4.24801244 -1.42050022 -0.00692815]\n",
      " [-2.96856091 -1.89745509 -2.91706368 -2.47802971 -0.02180427]\n",
      " [-1.29878952  2.00543255 -0.63545946 -1.64080561  0.1968879 ]\n",
      " [-1.66992084 -1.10030085 -1.84282972 -0.78540801 -1.31714587]\n",
      " [ 2.89525541 -0.82354792 -0.81903185  1.90158524 -1.08850368]\n",
      " [-2.78971285 -1.77983379 -5.21950069 -3.6583993   0.61183637]\n",
      " [ 0.79806586 -2.31893721 -1.71021427  3.12661299  1.28543738]\n",
      " [ 0.77630397  0.21244488  1.25800943  3.04184783 -1.66867068]]\n",
      "(10, 2, 3)\n",
      "Testing affine_backward function:\n",
      "dx error:  5.399100368651805e-11\n",
      "dw error:  9.904211865398145e-11\n",
      "db error:  2.4122867568119087e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "   \n",
    "\n",
    "af = affine_forward(x, w, b)[0];\n",
    "print(np.shape(af))\n",
    "test = lambda x: affine_forward(x, w, b)[0]\n",
    "print(test(x))\n",
    "\n",
    "#test = lambda x: affine_forward(x, w, b)[0], x, dout\n",
    "#print(test)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "#print(x)\n",
    "out, _ = relu_forward(x)\n",
    "#print(out)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "#print(*x.shape)\n",
    "#print(x.shape)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout) #loss(x),x,dout\n",
    "_,cache = relu_forward(x) #cache = x\n",
    "#print(cache-x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "#print(x.shape)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1: \n",
    "\n",
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error:  2.299579177309368e-11\n",
      "dw error:  8.162011105764925e-11\n",
      "db error:  7.826724021458994e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.999602749096233\n",
      "dx error:  1.4021566006651672e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302545844500738\n",
      "dx error:  9.384673161989355e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "5.1158526304107e-08\n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.22e-08\n",
      "W2 relative error: 3.31e-10\n",
      "b1 relative error: 9.83e-09\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 2.85e-08\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 7.76e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "#print(scores)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "print(scores_diff)\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y) #(N,D)=(3,5) (1,3)\n",
    "#print(X.size)\n",
    "#print(y.size)\n",
    "correct_loss = 3.4702243556\n",
    "#print(loss)\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"y_val\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 2.304060\n",
      "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.089000\n",
      "(Iteration 101 / 4900) loss: 1.845071\n",
      "(Iteration 201 / 4900) loss: 1.898521\n",
      "(Iteration 301 / 4900) loss: 1.760559\n",
      "(Iteration 401 / 4900) loss: 1.508818\n",
      "(Epoch 1 / 10) train acc: 0.379000; val_acc: 0.409000\n",
      "(Iteration 501 / 4900) loss: 1.512539\n",
      "(Iteration 601 / 4900) loss: 1.693669\n",
      "(Iteration 701 / 4900) loss: 1.763403\n",
      "(Iteration 801 / 4900) loss: 1.717784\n",
      "(Iteration 901 / 4900) loss: 1.535640\n",
      "(Epoch 2 / 10) train acc: 0.459000; val_acc: 0.438000\n",
      "(Iteration 1001 / 4900) loss: 1.490249\n",
      "(Iteration 1101 / 4900) loss: 1.449634\n",
      "(Iteration 1201 / 4900) loss: 1.737884\n",
      "(Iteration 1301 / 4900) loss: 1.515983\n",
      "(Iteration 1401 / 4900) loss: 1.319564\n",
      "(Epoch 3 / 10) train acc: 0.469000; val_acc: 0.437000\n",
      "(Iteration 1501 / 4900) loss: 1.403043\n",
      "(Iteration 1601 / 4900) loss: 1.444379\n",
      "(Iteration 1701 / 4900) loss: 1.410516\n",
      "(Iteration 1801 / 4900) loss: 1.479672\n",
      "(Iteration 1901 / 4900) loss: 1.551636\n",
      "(Epoch 4 / 10) train acc: 0.483000; val_acc: 0.458000\n",
      "(Iteration 2001 / 4900) loss: 1.461097\n",
      "(Iteration 2101 / 4900) loss: 1.441990\n",
      "(Iteration 2201 / 4900) loss: 1.369149\n",
      "(Iteration 2301 / 4900) loss: 1.462184\n",
      "(Iteration 2401 / 4900) loss: 1.430566\n",
      "(Epoch 5 / 10) train acc: 0.490000; val_acc: 0.476000\n",
      "(Iteration 2501 / 4900) loss: 1.476683\n",
      "(Iteration 2601 / 4900) loss: 1.427611\n",
      "(Iteration 2701 / 4900) loss: 1.250876\n",
      "(Iteration 2801 / 4900) loss: 1.420388\n",
      "(Iteration 2901 / 4900) loss: 1.393440\n",
      "(Epoch 6 / 10) train acc: 0.502000; val_acc: 0.496000\n",
      "(Iteration 3001 / 4900) loss: 1.398275\n",
      "(Iteration 3101 / 4900) loss: 1.446107\n",
      "(Iteration 3201 / 4900) loss: 1.363879\n",
      "(Iteration 3301 / 4900) loss: 1.376417\n",
      "(Iteration 3401 / 4900) loss: 1.538745\n",
      "(Epoch 7 / 10) train acc: 0.515000; val_acc: 0.467000\n",
      "(Iteration 3501 / 4900) loss: 1.444689\n",
      "(Iteration 3601 / 4900) loss: 1.238992\n",
      "(Iteration 3701 / 4900) loss: 1.461598\n",
      "(Iteration 3801 / 4900) loss: 1.335497\n",
      "(Iteration 3901 / 4900) loss: 1.325894\n",
      "(Epoch 8 / 10) train acc: 0.500000; val_acc: 0.484000\n",
      "(Iteration 4001 / 4900) loss: 1.399722\n",
      "(Iteration 4101 / 4900) loss: 1.530197\n",
      "(Iteration 4201 / 4900) loss: 1.363651\n",
      "(Iteration 4301 / 4900) loss: 1.300791\n",
      "(Iteration 4401 / 4900) loss: 1.479261\n",
      "(Epoch 9 / 10) train acc: 0.538000; val_acc: 0.478000\n",
      "(Iteration 4501 / 4900) loss: 1.177159\n",
      "(Iteration 4601 / 4900) loss: 1.395358\n",
      "(Iteration 4701 / 4900) loss: 1.164627\n",
      "(Iteration 4801 / 4900) loss: 1.190451\n",
      "(Epoch 10 / 10) train acc: 0.520000; val_acc: 0.480000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "pass\n",
    "#model = MyAwesomeModel(hidden_size=100, reg=10)\n",
    "solver = Solver(model, data, update_rule='sgd', optim_config={'learning_rate': 1e-3}\n",
    "                ,lr_decay=0.95,num_epochs=10, batch_size=100,print_every=100)\n",
    "\n",
    "solver.train()\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56481a5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3040598883939922, 2.300446193720234, 2.294973475199162, 2.2878023970351107, 2.280392357079912, 2.27898334505759, 2.256002096112359, 2.2486812109042975, 2.2514855548324384, 2.222607604265382, 2.203231869955518, 2.1496090125736527, 2.120779260340083, 2.1696354545419148, 2.1595909056701754, 2.1085518206730725, 2.095817510572407, 2.1331581849249814, 2.1288446891464834, 2.149046040246565, 2.0731249248788606, 2.0094928871156297, 2.0773061662318946, 2.045761969826064, 2.196720619460295, 2.1087727736344046, 2.048728933749318, 2.1569947002959915, 2.085718762384377, 1.9701478733599054, 1.9838826111757197, 1.9977672340868167, 1.965082602140133, 2.0809968447804272, 2.1053174407832826, 1.9605848912767696, 2.0975212376198793, 2.0214646459322436, 2.0025223377619725, 1.972306855050787, 1.9446688207667913, 1.8991042331278831, 1.8286378131454322, 2.1536260378240275, 1.9234149080903415, 1.7245433781702217, 1.9425375563369618, 1.7848061405352689, 2.033698587721984, 1.9209537785964006, 2.0374838285653247, 1.951118395173249, 1.8859134351958176, 1.934239691770605, 1.792416739734448, 1.8360006978963208, 1.9668751579931694, 2.0337313746976267, 1.913923309880006, 1.8820636436042313, 1.9150770449323329, 1.929761246419511, 1.8355865467744932, 2.0211965032484103, 1.9319305682032533, 1.8979684137406363, 1.8896386701162067, 1.8090534288319962, 1.9649985195096407, 1.7982202426254452, 1.918305963541552, 1.8182570259052506, 1.917521382573087, 1.7618832062004786, 1.6918047739793922, 1.9242522964311712, 2.000276777480126, 1.87540770018449, 1.74942639718458, 1.8978621669502664, 1.8873311441860392, 1.8456529736411078, 1.8262597955989353, 1.8664864256405023, 1.8950237859103245, 1.679436456833127, 1.9217394356875213, 1.881649130626115, 1.826467824753635, 1.9400093396404707, 1.7671198909515535, 1.7496290576080489, 1.7428280314438658, 1.7883919216381983, 1.7911094324933117, 1.8038234589021658, 1.8251343712192545, 1.6918919513262012, 1.8354085577531225, 2.0392575079959236, 1.8450705818358744, 1.8609353904966917, 1.7974274552769347, 1.932780112779703, 1.8113424502004145, 1.5995169741385433, 1.9007102886524692, 1.7332788513189983, 1.8493976099414349, 1.9481684825080194, 1.6580862868377968, 1.8550725766214424, 1.75383114925606, 1.6810938515433582, 1.8761680118809974, 1.7694904661108886, 1.8310294591159733, 1.743957070659184, 1.8482208442050538, 1.7527371581730489, 1.7776054804935009, 1.8638735335701997, 2.0004978037983427, 1.7578662171980801, 1.9038210492526457, 2.0230320264960415, 1.8629723824851263, 1.7968854608170113, 1.9330393485162147, 1.6311399327272351, 1.7444433371965562, 1.7680023379590901, 1.7150102230163304, 1.7280625103569502, 1.5611869856045262, 1.623959740731599, 1.851953826625646, 1.7184119813969732, 1.7846675898614075, 1.8004180276728277, 1.8095650947091189, 1.6914416125370895, 1.7292703791473165, 1.7637302851197878, 1.651143242804917, 1.7782081986823712, 1.789996102228103, 1.562980750374907, 1.6141904424221338, 1.7640320418596405, 1.6436880870785897, 1.762602566829444, 1.8439860663766086, 1.7384298133459979, 1.6981051066896657, 1.7706484401063214, 1.830156996536091, 1.883260354060792, 1.7434961676695042, 1.6225101895821061, 1.8021851200229224, 1.6873643495536153, 1.8107140279289569, 1.8007157478910145, 1.720508509352375, 1.8544677953318418, 1.6761072111169966, 1.6651775411797192, 1.7469802324218129, 1.751452446556045, 1.622876864529418, 1.758135589115607, 1.6578893151743004, 1.770782435919896, 1.6294918563689267, 1.711397782676743, 1.7079815039384605, 1.6019652680551593, 1.7349891438851395, 1.5970499581203155, 1.6381658617778418, 1.6620661097546305, 1.7159362793466653, 1.6075316341599983, 1.7697957070891561, 1.636544870563663, 1.6220918444729346, 1.7209065314810632, 1.8249060002554094, 1.6731263548970308, 1.6826278572379547, 1.662559583347231, 1.8229168433381944, 1.7041166160959254, 1.7373818002642218, 1.7682222557084228, 1.7046900876711155, 1.7077644635284077, 1.724743875537463, 1.6503433099556377, 1.898521022653523, 1.6875549033406358, 1.7896868202069995, 1.7933462614635558, 1.73701598938022, 1.5837297991697739, 1.8816958947382858, 1.7957256642875463, 1.7681000551916766, 1.7900419669559817, 1.7108892525958663, 1.554237632899047, 1.6295558885472832, 1.6324388540818218, 1.6974228075309876, 1.5697412416073573, 1.6671746185584306, 1.7842469819620819, 1.7743959572295416, 1.5884718760370589, 1.8010208654416058, 1.7757478034753036, 1.7311476553002851, 1.6617751833607692, 1.7903748368124408, 1.8146263381621892, 1.6610857065073272, 1.768573028067772, 1.6847007280709374, 1.7071972561115494, 1.6323496172052503, 1.616649749922584, 1.7510914628599838, 1.5826101616025965, 1.7804552218982774, 1.6554437292128281, 1.7648294098923234, 1.682506501693529, 1.7059541880120963, 1.6382343858365178, 1.55642904457354, 1.5681705336781118, 1.721677829004521, 1.7403355250330272, 1.7957830380926671, 1.5520442752494361, 1.8147679016820462, 1.4721316741707458, 1.753082895265716, 1.6630990923121833, 1.4759414601581355, 1.7263496408487438, 1.629516902501151, 1.6650157336652256, 1.675033606009292, 1.6424965602190063, 1.6982646896860802, 1.7423214748767122, 1.5278290790349434, 1.4344460180807281, 1.788715336599453, 1.7626605910585482, 1.5671346234025858, 1.8171987678482728, 1.6494717048369458, 1.7703263198331605, 1.6166385434647623, 1.559328628075238, 1.764110598020139, 1.6836259776788378, 1.6806111628731357, 1.5909130725584781, 1.635025564475478, 1.5830175065341177, 1.5612536559263668, 1.5707675724770744, 1.6197196473783046, 1.6328775356391767, 1.7413590740989933, 1.7859168094594975, 1.684395644475377, 1.5477764915093324, 1.624474015523314, 1.6171205878616677, 1.7342757126854238, 1.750369555856808, 1.766045608501571, 1.670593111762799, 1.5989647916717138, 1.6603169390255812, 1.654315198602407, 1.664903783850488, 1.7936538559993482, 1.5722851705027399, 1.650026151730778, 1.632844579436667, 1.697081033655249, 1.6735449532516364, 1.602139558304037, 1.5227007879662133, 1.760559472133994, 1.5691010476446123, 1.8303488299163353, 1.7623157056944487, 1.5787550089831845, 1.767999937014227, 1.806366835073225, 1.5315198813627853, 1.5459900521703214, 1.7228941530966109, 1.5574094948585022, 1.6467242017206862, 1.7883679277719386, 1.607814278349377, 1.7463141044650305, 1.680152521253289, 1.5879808906156088, 1.6533766699367882, 1.5555959742893197, 1.7612826599902436, 1.5517235131615874, 1.5571416652257586, 1.5143324296565124, 1.811303769191355, 1.6072929790083186, 1.7500003657623342, 1.6987015804970549, 1.9189841109800483, 1.589864959396932, 1.7160301284008324, 1.5331425650765735, 1.5654898433985536, 1.9294517105181317, 1.6966208343024716, 1.7327156152029592, 1.7446304350061175, 1.8359958726031917, 1.7301495667054747, 1.6144961860614688, 1.6255524267300223, 1.7902859054723144, 1.5749717212508727, 1.6238228932766647, 1.6756397061230413, 1.6840780633862713, 1.7281344973734876, 1.6782077106432083, 1.7070114053613468, 1.7009905080493355, 1.5612716477995414, 1.663807630147946, 1.662143851676385, 1.4940254570331997, 1.7613411155582481, 1.747264014149109, 1.7286049265662116, 1.6005732826162342, 1.4717068573868917, 1.6454036624661403, 1.6310454135300885, 1.8608045389570933, 1.7043931453947485, 1.7318993842169932, 1.7421597701508897, 1.5517176789426117, 1.555242879297161, 1.5909731187643155, 1.576119474061169, 1.4903587337524373, 1.555526051765808, 1.7554820465310599, 1.6881462470546384, 1.6126362557637288, 1.5251571752604687, 1.5997028283320418, 1.5611450787961616, 1.6370277816190881, 1.6775720188234273, 1.5858141328829505, 1.6454801950066038, 1.6796510487240999, 1.72323606069279, 1.5264490398173782, 1.629897038392119, 1.6657507423124889, 1.5510358845211993, 1.575062622163502, 1.722885709478167, 1.5901071842384593, 1.5598534777325646, 1.6024121642977018, 1.6604109242826466, 1.6725479458151538, 1.7892219501933524, 1.508477874471795, 1.7055623275592517, 1.648903315015523, 1.5964393816998956, 1.5529375608657718, 1.6931781185458112, 1.5088183186661346, 1.5582208915967704, 1.5655800344027526, 1.7196777322844403, 1.6562379962588119, 1.5747714238257153, 1.6487693828778345, 1.756525003746041, 1.6525676988814404, 1.616276992458411, 1.6371122389848667, 1.4931445286415255, 1.590590644311274, 1.5093156561460757, 1.581598653895182, 1.7293752666247562, 1.4425436802470388, 1.630482945776053, 1.7561231056009412, 1.7246211894077128, 1.5027094130409924, 1.4919639766422916, 1.5833632540831757, 1.5228837245837326, 1.6788127350183735, 1.5144119169315606, 1.7295158788326974, 1.7698968377503064, 1.688808505416518, 1.5890193627816829, 1.5814408614888809, 1.821361392931821, 1.5642623370287772, 1.702595900173871, 1.6728865548801093, 1.4356427634588789, 1.4635725862179323, 1.8626098744377342, 1.4403929846397543, 1.6129113646440882, 1.6562554494928177, 1.7252292822265358, 1.5221557341108505, 1.7410298298878215, 1.6497254442852585, 1.6303566386857264, 1.7171498407195538, 1.5152551430995376, 1.6997734656735353, 1.4556301436006998, 1.6673467719923838, 1.7029137406424075, 1.5641898471601892, 1.4725271856869009, 1.6128504424121144, 1.7434408767375351, 1.4986786835723604, 1.6738134030034346, 1.5122745599519483, 1.5863973424783209, 1.5023383424843784, 1.5380349250887557, 1.5824906191854344, 1.736918514651934, 1.8524165451160803, 1.7779931643671574, 1.602532645193387, 1.430058083435278, 1.6444346326405994, 1.6319488747835131, 1.5364198745840627, 1.6848143837184983, 1.6352874572637865, 1.585284297708236, 1.52976483554295, 1.584505077668802, 1.4758645976583582, 1.7491471082892742, 1.5544465137022734, 1.6482190113292514, 1.4881444461262618, 1.4879263749571658, 1.3678065037415188, 1.6884782683910462, 1.576025793289433, 1.4606565814347576, 1.4442814315242996, 1.6124369873872388, 1.5069366496000385, 1.4917582747695741, 1.6638378964890421, 1.6614573903758363, 1.5865981182527948, 1.5448703322942299, 1.669079739677867, 1.7491137749107815, 1.506856725656205, 1.648518527463419, 1.796195790847252, 1.625418333402762, 1.5125390063949629, 1.6985516565993541, 1.7342060036483895, 1.5962761877595883, 1.6688785323725843, 1.6389617047326641, 1.5208856721633175, 1.6536047582292983, 1.6507115072207605, 1.5606194169980452, 1.4720201355970657, 1.5723088928186126, 1.6335725796965206, 1.487133678881046, 1.508186051322247, 1.6353190666642201, 1.535549344431442, 1.6731628595786368, 1.571192580123556, 1.5489086702077504, 1.705837244012503, 1.7246881538311996, 1.6137078779411929, 1.584489610826056, 1.5080293135128582, 1.625918867895324, 1.2834936313467438, 1.6801444527757872, 1.633715464594692, 1.4828547548481188, 1.7410708170357287, 1.3929640089786046, 1.7098004729939873, 1.4925798149889535, 1.7451996188219439, 1.6394623163090714, 1.7368524503174538, 1.6832305219018757, 1.709951579358604, 1.4491185174888352, 1.547089848709569, 1.41172401212935, 1.7199306410255095, 1.5714780703720437, 1.5450797272821482, 1.4845776967220456, 1.504974687883407, 1.462923924783912, 1.6197124708984445, 1.700446186427358, 1.5641256394527931, 1.6416062609408775, 1.6067884460980064, 1.6545116201570567, 1.6293338507071773, 1.5030813201668125, 1.7959938266914113, 1.5068511397333821, 1.5597051981807846, 1.479927942976148, 1.4377718724245918, 1.4624869973305363, 1.678113039002013, 1.4673863353668326, 1.4819405598844406, 1.739048058251052, 1.6822916872377751, 1.6225330254311723, 1.6465910908007888, 1.8041333635168533, 1.6202900949182055, 1.6840439307960946, 1.6368883122325035, 1.6860031376887648, 1.6563832027896057, 1.5971602574635477, 1.6718927809502078, 1.502916099093185, 1.4461389501304784, 1.4906118631568028, 1.5435711361190232, 1.523074919733106, 1.8084984223118747, 1.642800544250666, 1.5287966919428555, 1.6782608060216544, 1.602150231976517, 1.372085254040835, 1.470427622101572, 1.6908286913846706, 1.7003835397201699, 1.472287033484754, 1.695107225970074, 1.6016576851801525, 1.5977859723498937, 1.6120154864549476, 1.7010128387218832, 1.5791686877489233, 1.6272856557486564, 1.6899933535798066, 1.6936694170261501, 1.3948905167375227, 1.7256487188362328, 1.6141169739323513, 1.4499460468801866, 1.5092807628500091, 1.6150756556720607, 1.6377600730930757, 1.5301283647415138, 1.5942917156642529, 1.5543976766870002, 1.679162292850669, 1.5382685953404893, 1.7394266885565726, 1.5609793832794256, 1.5399186624005878, 1.4650675428966786, 1.484779326679839, 1.4793970469965472, 1.5333485880048006, 1.467679188486333, 1.6179104591095834, 1.6733255223419403, 1.688509059224337, 1.5285625201554813, 1.9084468300511472, 1.6581987927863546, 1.5644654178321356, 1.5745268245858284, 1.4949845966005118, 1.530721642289882, 1.7842320430041776, 1.5702498695957792, 1.7407275275686402, 1.5668584860724684, 1.391988606010965, 1.6474528495635985, 1.4631092791574705, 1.4559240621546534, 1.5722827974947504, 1.5621641375169781, 1.6037424352298115, 1.4480714353695787, 1.7144157270578675, 1.5386118401842077, 1.517995620714045, 1.539646450459702, 1.5534267096039909, 1.7305439416614246, 1.6031705440214654, 1.5417317368188066, 1.5326027416022663, 1.616492036443527, 1.5112180684432244, 1.5387182104951709, 1.8095742582261307, 1.6632232258166806, 1.5868668315266903, 1.5478805591836886, 1.7059077274714383, 1.6069786679326992, 1.5156535281940848, 1.5545211497495033, 1.722033353819526, 1.5428616683367613, 1.6335737625806812, 1.5743917597703123, 1.5677310902345547, 1.679023460591769, 1.6053831400235063, 1.3089602547968733, 1.656202874780019, 1.4946969752174883, 1.5325527881406893, 1.457948737280611, 1.6168460919264533, 1.8392206532675297, 1.5326091404895776, 1.504759319962267, 1.7241713696834713, 1.7013448357295542, 1.456027567257154, 1.6002790991421498, 1.602194705951467, 1.4187907161958169, 1.5283131254842075, 1.617278106170005, 1.539068035204709, 1.5829902038192558, 1.6109060843953527, 1.6201062982321306, 1.5486720867276933, 1.6557149205422421, 1.5871920654776905, 1.7346736605053699, 1.5774245790087904, 1.5772958088274907, 1.6492058827300102, 1.5901052786641596, 1.6519362724178137, 1.7634033048321018, 1.5817722895835902, 1.510155864967053, 1.4919055297933048, 1.437743030862069, 1.3592088479613358, 1.4070397798491143, 1.7210168136124953, 1.7613316986254106, 1.4483149163785638, 1.707561690912117, 1.4826464545902938, 1.611935731290845, 1.4799064239776971, 1.5874156468123244, 1.402238118354991, 1.6019895661075412, 1.5145956566671486, 1.6103075414283865, 1.6157429172784645, 1.5624248589921348, 1.6809081051352799, 1.656001197560604, 1.4565857699076223, 1.3678299322867948, 1.482918118645611, 1.6743916379882537, 1.5004632292545101, 1.5203632908568172, 1.5821254909852416, 1.761653814771183, 1.7251086386564174, 1.412937342688705, 1.5921740655644054, 1.5684571320851435, 1.5794787102253156, 1.5684529959807252, 1.5107836441043414, 1.647727023847797, 1.3372716906276225, 1.4785425331398334, 1.482678913249369, 1.7028413935143205, 1.5847696267922664, 1.5267051361700277, 1.6791014722272146, 1.4587322849363429, 1.3288153081861855, 1.4840018881218808, 1.5398505716444197, 1.6541222007845322, 1.5387488016330875, 1.7577571029063406, 1.4662434097684005, 1.5488080987797168, 1.558494270551547, 1.4562111828131692, 1.6896562082791104, 1.3830817485468305, 1.4918912198485146, 1.61480379588953, 1.6393073676533263, 1.5784693295957135, 1.580459005307582, 1.6127063580696233, 1.6956546089425677, 1.5068733858363472, 1.4646942864843322, 1.5144038317555126, 1.5639610728974296, 1.5693724985902964, 1.5037467738603327, 1.3987833227269846, 1.6756868683841473, 1.4618410139300257, 1.6455963804769715, 1.6779836323594475, 1.514976687242328, 1.5230806193912136, 1.6528959520587398, 1.5914554778765053, 1.561040564439781, 1.4991524185195664, 1.5660525547351576, 1.7765783206826844, 1.6505465369539019, 1.439230020731319, 1.5284276802610721, 1.7105192710454682, 1.5422826239166205, 1.618011266439585, 1.5367063101038176, 1.6411589356916096, 1.5353947997296888, 1.627735968777786, 1.4558037637021832, 1.5753331811414413, 1.597495748694425, 1.4786339629743404, 1.5152932038183895, 1.717783976353441, 1.6384759937569142, 1.489018723274187, 1.5030607367394109, 1.5426855409317284, 1.617877407376221, 1.4891729043272668, 1.3937350867314118, 1.503022222263068, 1.6729457331606477, 1.615044458491327, 1.4728949836668404, 1.4680897877753822, 1.663061707672994, 1.6935291720781793, 1.5321812161365151, 1.4093094370011565, 1.6056332288563906, 1.7040467936781578, 1.2378908717737374, 1.505106827809237, 1.546788893937697, 1.596004256088333, 1.8186856205638906, 1.5955140421696945, 1.4454729112308704, 1.528364008686491, 1.4628111657625453, 1.5061308090113181, 1.574875013969071, 1.6489181779755913, 1.5958159394464997, 1.5663125096423252, 1.5993370195762402, 1.5221734491230268, 1.5355031866770223, 1.6002248755900166, 1.5538847157188118, 1.5839143496918604, 1.5651859911811812, 1.44549107316771, 1.6424166149652855, 1.6319950755550232, 1.4181310159442637, 1.473047849860268, 1.6839213339960724, 1.6054764241669073, 1.4885062411963605, 1.3347778638877605, 1.612849285121667, 1.7466771941011323, 1.366052468305627, 1.646731040768803, 1.4766170910113958, 1.455494356202765, 1.567658129779627, 1.4738437373124826, 1.5323568446558051, 1.5487526641725256, 1.4194409477223362, 1.4121007301752397, 1.492773452590918, 1.6536132766757543, 1.5292806971946593, 1.6831726444299653, 1.6767814439935853, 1.5236366296412232, 1.7536185470829855, 1.5364681286752622, 1.5232783627163868, 1.602345952418417, 1.452496198315232, 1.6466761435172046, 1.6726684064451975, 1.4445162584516031, 1.5415318110178966, 1.5285153562921225, 1.6338009208819146, 1.664556503149681, 1.5291462543360723, 1.4742553836981422, 1.4561576022976626, 1.565049815575847, 1.38734351448327, 1.6304753311213795, 1.6112390714823086, 1.3016845568457447, 1.588192388022589, 1.4098938432436103, 1.337923043522093, 1.5835256020984998, 1.478018061736118, 1.5363677304294694, 1.549934711486143, 1.5437596140236312, 1.3477483618733683, 1.4808179827070092, 1.4897923688865198, 1.6419054273004228, 1.5167500677282124, 1.5356396197886022, 1.5206976583116572, 1.3980079547639424, 1.5278008930711742, 1.4252498202333457, 1.6318030293583337, 1.5562472477045801, 1.5872378793396538, 1.5029902131365622, 1.3314591807290528, 1.3700254125870674, 1.6264969874229502, 1.434167550113923, 1.475303507846445, 1.444790854389418, 1.454648907332844, 1.5238693554278484, 1.708746561044599, 1.5945747329891713, 1.5938596985126865, 1.5748780784977905, 1.3947132863027047, 1.632731823130005, 1.5723745544393224, 1.5645549937488314, 1.6253443771386886, 1.336174718992728, 1.7888457793988053, 1.6606135832456883, 1.6783406436152657, 1.626103728323232, 1.3924324068371134, 1.5260942684883099, 1.591129540993342, 1.5665851571362497, 1.5431102440787041, 1.549472782713509, 1.5910365451052213, 1.681151749639698, 1.4714800360103022, 1.5198178778267057, 1.4383995506323177, 1.56459740732363, 1.4589160076926788, 1.552681410083881, 1.6056634834104329, 1.68588570187085, 1.6352469515628525, 1.568446998058918, 1.5787753632150818, 1.4692640810860877, 1.6480909000308746, 1.2973321246000709, 1.7472722193667856, 1.4906803669854531, 1.3654307816795295, 1.334741740068341, 1.48084771425811, 1.486679991516642, 1.656722745588874, 1.4855163871293464, 1.4089993889309358, 1.4950584399415834, 1.6556192716535045, 1.4386517684980102, 1.578177053391721, 1.6987982859970372, 1.612487526214911, 1.5600682252002291, 1.419353055925559, 1.6107177848680263, 1.6752712867017805, 1.650917119373625, 1.4932659111894537, 1.479415014263925, 1.5174741138803083, 1.5606326348762343, 1.3902073551355374, 1.4900683041002372, 1.484483993712918, 1.5683414822035167, 1.463800418061395, 1.573693767727047, 1.589473537216604, 1.3976766177714364, 1.532379318019608, 1.4851194176047662, 1.5136542645862952, 1.4753166668443445, 1.4313686192056985, 1.4784287170486765, 1.4141307693020142, 1.5432353024756937, 1.5465718547202822, 1.5540557612126162, 1.484171647472451, 1.5842940108088448, 1.5152894802175285, 1.6213343316725988, 1.4597999846999086, 1.4902491809703602, 1.5850781162838268, 1.4116687974930853, 1.5361102776462543, 1.5451842167570624, 1.540066489777041, 1.685965600537512, 1.6586608450251672, 1.4271993833390944, 1.5306148749017114, 1.465683353858463, 1.4654900240775377, 1.6208063848774157, 1.4726540624480233, 1.359798198901911, 1.6068311055872684, 1.4921679746795957, 1.3600816519496917, 1.553242728394024, 1.5467109043973089, 1.6484198161740278, 1.539653675224638, 1.5078712404118184, 1.4844978573174257, 1.4581109927272664, 1.4540688130567734, 1.375562555049821, 1.3766175151270952, 1.5911419832799079, 1.4751960802162136, 1.6142834924614347, 1.4394663753767105, 1.5471811444203019, 1.5569145245897633, 1.8334861345526017, 1.5124117436596094, 1.5090176823251338, 1.461557960216704, 1.423464279356652, 1.5470818152886125, 1.4365203324900628, 1.6451962197465873, 1.3215388357517375, 1.451581361336414, 1.489628655333942, 1.5022666033027121, 1.6259033874258264, 1.5300026088605896, 1.6651481930980032, 1.4384627558940628, 1.3936154272118357, 1.1625258019258433, 1.7277226404767547, 1.6120405347549243, 1.4175872819955404, 1.4902069963903508, 1.445960936957102, 1.615189811070927, 1.4504057784971471, 1.5201295636921819, 1.7213990453590744, 1.5552208906208949, 1.4108484806904626, 1.3885150400180302, 1.4892500819418775, 1.2560669059182785, 1.4925161188255531, 1.3755197778423947, 1.6455019475759085, 1.411703728027972, 1.5587239002920956, 1.4790772353691866, 1.5341641104289006, 1.450963282975935, 1.3356326057289483, 1.4570782281960113, 1.5041635065789918, 1.3328035096819233, 1.3423917446801639, 1.6301290292851633, 1.4204626984661386, 1.668042300571572, 1.4338999498721492, 1.6326902033620658, 1.7195963082785923, 1.4674724848767804, 1.4943342360426422, 1.7358404721535305, 1.5489231223694473, 1.6099555524524427, 1.5117084641728533, 1.4912894504401606, 1.605774960356281, 1.250009740896462, 1.4477771912983814, 1.5957846199175736, 1.5074390947732803, 1.6207624613917675, 1.6078847397442535, 1.5651452994193897, 1.4496341124826744, 1.4604557534347753, 1.525949970202387, 1.49721328805932, 1.5627653201846992, 1.442767038083351, 1.5075839817910262, 1.5692942644009915, 1.645275905983351, 1.5894351049976103, 1.5558781970446816, 1.5594528905859315, 1.5203944664210267, 1.3459573010642478, 1.6275985797252952, 1.4980273512901556, 1.5407440818650417, 1.3791275362174054, 1.4633279592420585, 1.4661200447122218, 1.6214865825800429, 1.5247123283922162, 1.4982206560085936, 1.3253162377191554, 1.5957229416876981, 1.3909027322459977, 1.55186562883743, 1.4883042826402555, 1.5487870319326442, 1.6784642394389448, 1.6301319636411353, 1.6244444692544207, 1.4538693321530376, 1.3762253709054533, 1.5957184395467012, 1.5782883290529177, 1.4842563799429858, 1.7749384482902024, 1.3244539202102672, 1.4090027581274749, 1.4637890912746672, 1.6825099552324483, 1.4828900318963116, 1.6276678271673872, 1.341319674041958, 1.5148919042178843, 1.4034701115983717, 1.5656721491128487, 1.5242301630000117, 1.4926072134285762, 1.4087875807815848, 1.492183511320654, 1.3989237166528026, 1.6762400734701857, 1.501139931089121, 1.4335548321824274, 1.5868057070708053, 1.4623987719145837, 1.6355413108169847, 1.5362780224215988, 1.5043143495792588, 1.64088138660683, 1.394805501690241, 1.3075409572434737, 1.5048469916538798, 1.6205227424888924, 1.4652562656958796, 1.631649891547993, 1.4305331295713972, 1.5629788894841026, 1.5078382602039972, 1.4169691617267484, 1.3107727328809187, 1.6327439844350504, 1.539534028109995, 1.4870830516415257, 1.4966200542297097, 1.5349631228071425, 1.518744354710293, 1.5624634211025734, 1.6277001156221977, 1.4924710373279644, 1.5589495471905161, 1.5954796565562088, 1.457776743442232, 1.4674207044951262, 1.571944256543309, 1.482556158350566, 1.495893140761881, 1.3606592272046438, 1.4031531091709235, 1.7412080748261507, 1.652102483574767, 1.4293652831332215, 1.2798689984869924, 1.3853548548952563, 1.5602479273352463, 1.4859189387374412, 1.542626455150221, 1.4096784285967972, 1.7378841374577838, 1.6354233728294394, 1.4597759546206526, 1.6086220016679769, 1.5095284478255067, 1.4189011929459603, 1.4248107309450555, 1.6904766354448255, 1.4137167117624605, 1.5218777602994478, 1.540510394844973, 1.6787424359588914, 1.4828644747478104, 1.5547833171646182, 1.611872905876591, 1.3092323411857496, 1.5358595766949505, 1.6235241393474913, 1.4219819506858493, 1.5791707934639305, 1.5229225752490272, 1.3408128909180872, 1.4443886117071347, 1.548232416201862, 1.4346513643936927, 1.3794954724128794, 1.4243835478032725, 1.5033376215984482, 1.6052870419401777, 1.4861875432844522, 1.5442907029784978, 1.3808123040338003, 1.4526585862088937, 1.6802286474442536, 1.6705499693492436, 1.5163176612225824, 1.4505817439718824, 1.4125425788318469, 1.4760778255965596, 1.5673227010347295, 1.470462021936718, 1.583706739656029, 1.5676026320789689, 1.5727975021925258, 1.5251515503623516, 1.4762193551658411, 1.5295628407008153, 1.5282955899178818, 1.6120412906689094, 1.6456434702681295, 1.53150697753047, 1.4838373623631471, 1.4224971735471246, 1.7918889120465418, 1.6943052374948717, 1.5423048309781502, 1.418995727472433, 1.603825586132938, 1.6774541963169398, 1.5739408071484204, 1.627374480049531, 1.445369555266297, 1.6259646782429837, 1.2259607412246671, 1.46028109512212, 1.394947526547358, 1.5503185993756696, 1.4370170266886222, 1.5104165731676957, 1.4180865004588004, 1.4332924081879281, 1.4935322851559725, 1.535984308919868, 1.3804536271093104, 1.5222035803033853, 1.3308700782427, 1.592925370523077, 1.4412185594683413, 1.4694040508733142, 1.6172298423896163, 1.6187660076745827, 1.5732675783666894, 1.6142301392461897, 1.520269035377583, 1.5718245496818009, 1.5891521771429264, 1.6220782073806292, 1.588947338042574, 1.4219491884804127, 1.499756467290124, 1.4595596029649005, 1.2771284519275365, 1.5538625147375058, 1.592484178672804, 1.4548469667676929, 1.4541926086379384, 1.476814885681982, 1.4758721193304307, 1.5601156939499046, 1.4687696800829766, 1.5159827775487016, 1.4622167207062426, 1.689149227644613, 1.458319947904532, 1.4195874444643692, 1.4648760228293187, 1.6071197380339655, 1.4899574017286707, 1.6396829646903825, 1.5074014316741695, 1.3842337129527713, 1.4325008378695199, 1.4965072233267815, 1.3947113002464095, 1.4915642237365985, 1.5762070953683445, 1.4215994355024781, 1.5556318888540455, 1.4328422020910236, 1.3612175228088341, 1.582209874774574, 1.42295084036975, 1.4277130274761294, 1.5761435568042694, 1.6169423010760298, 1.5099551604388406, 1.3565433247332148, 1.4998843086874105, 1.6453972143582132, 1.4722804712330986, 1.476162762217622, 1.4804877800285983, 1.309774084902482, 1.4351616900567996, 1.6413178313824344, 1.5471312822545065, 1.4974827457195645, 1.5781943223032173, 1.5438918736962697, 1.5357035438664175, 1.4309786492740981, 1.6648366749452543, 1.5376219575774, 1.4223535149544988, 1.5130283996843537, 1.6191316826623818, 1.4095106509712005, 1.3258447671209732, 1.4106192433901046, 1.6797508544273247, 1.563234977362104, 1.304460439959256, 1.375594685272085, 1.5340492849234826, 1.5256837938768022, 1.4782448121964173, 1.39477422439904, 1.4559800155429803, 1.5926812896274187, 1.632586853967566, 1.4889401199477843, 1.5859726691664449, 1.4984785249417032, 1.6471125900544228, 1.4329330895430317, 1.4580944381351322, 1.5877861184463546, 1.4336140676138194, 1.5122024647981724, 1.476085299171177, 1.4603543732321864, 1.4445944408945304, 1.6537904024059285, 1.564860338436408, 1.4665574160271162, 1.4788798261202118, 1.5808623822469814, 1.6386845746234464, 1.4503228665758465, 1.5132628976815417, 1.442843038040252, 1.3831450817946958, 1.5328566125125638, 1.5993720062810204, 1.423987234307822, 1.4901366906457438, 1.459188343035317, 1.5526326078492747, 1.4506086735823478, 1.5319029780570983, 1.6260083993951329, 1.494140273091357, 1.5145132251439761, 1.380101846764107, 1.4839759890147999, 1.4851370147924994, 1.5336419273284256, 1.398442961328043, 1.4308465585461025, 1.4907734219591378, 1.319563580628349, 1.379366919375693, 1.6265852385919697, 1.4673562670578408, 1.6319201009024868, 1.5504047976167052, 1.2239788709664579, 1.5619014173991423, 1.2533492896591685, 1.3939286443573327, 1.4723579366388062, 1.5535634039849142, 1.5170726267429149, 1.3288691479134076, 1.4215759428914634, 1.4078682095784076, 1.3800006535295728, 1.5139202616446668, 1.5409869201072712, 1.3500352092510925, 1.5469827127898261, 1.6257422877364474, 1.6289613200929218, 1.4095142904817135, 1.688555126594559, 1.4862014378058126, 1.4315952931770388, 1.4685958357476039, 1.510439993780995, 1.4744052977629816, 1.6099633540258393, 1.6222081448247951, 1.6562951139938387, 1.3340558451036588, 1.3908928881557032, 1.2322757056223903, 1.511157463785028, 1.6128425507679303, 1.6730047427385546, 1.5249045984146792, 1.4048455463146734, 1.5551258493296194, 1.407246509710934, 1.619601423774756, 1.6224354493599051, 1.5490939438417073, 1.3932518514048953, 1.440831342213785, 1.4574886068249233, 1.469487414920816, 1.7679672374216198, 1.3897120658815094, 1.5927207984314546, 1.5081090565096118, 1.4896709558000938, 1.4498142281301096, 1.5004829443577778, 1.6344235766023496, 1.4195182770155301, 1.4475258472663035, 1.259143140359034, 1.502574939174377, 1.5769660481380379, 1.6386875237583995, 1.4350549601165057, 1.6887859575395519, 1.5128497488571586, 1.5613860646596538, 1.482338604015848, 1.4356041596232283, 1.4575429769130708, 1.5992022418398155, 1.3782190800870513, 1.4052156233975939, 1.5794749204812095, 1.5432156854111494, 1.6197134947507288, 1.3981043242886202, 1.414003779105956, 1.4422090709140238, 1.4170197203736472, 1.4740958165967044, 1.5376982310873613, 1.3734186194234161, 1.526226809853369, 1.4309748663590531, 1.4515171255685773, 1.530694486003286, 1.3663600887143987, 1.536390596827413, 1.397212955680813, 1.578787928217894, 1.3810716361400972, 1.4032407149992099, 1.3854157075428482, 1.4874572939963675, 1.424698582365952, 1.5278840048107531, 1.408000012922042, 1.3879967105597928, 1.4030427805880026, 1.4527741456418293, 1.424366096410898, 1.569222583185713, 1.622448245862025, 1.4805320718051758, 1.6176027443653869, 1.4095127747332454, 1.3534823925352728, 1.5407868374746903, 1.4368907012866488, 1.451689788142419, 1.4345385951044356, 1.3816730571637665, 1.5537619851133184, 1.6781690013784114, 1.4491282988747676, 1.5676177122205726, 1.612849933134008, 1.6890834130736956, 1.4743351365496802, 1.6144107029477124, 1.5144941412772772, 1.3863391922728134, 1.3160356631910681, 1.3766643596889097, 1.3824685747828114, 1.3924758639971777, 1.406509308995619, 1.50146274062666, 1.5476337853745974, 1.4195871891061094, 1.5535601640494536, 1.651918428052796, 1.3582345479279407, 1.4935335089744706, 1.4626301910735855, 1.3862901494772097, 1.5553421746689151, 1.4922856278524688, 1.3355106352048403, 1.4575216427129944, 1.447063017932486, 1.308591897216697, 1.4559803735424268, 1.4593297756993864, 1.4489361494973867, 1.6437433972373228, 1.316032672485488, 1.6811070599151594, 1.4254123220337744, 1.53346434987728, 1.5598764886727385, 1.4754998437296698, 1.4010233777117371, 1.1950443094871144, 1.4915116115830205, 1.4120315000012682, 1.222638441153157, 1.4435371324493784, 1.2644945197953497, 1.4767554256822906, 1.5277135274991391, 1.549342550089911, 1.5942491346568217, 1.5668693391900663, 1.5025272933358054, 1.4819981910815332, 1.4551887108168648, 1.4375103186788625, 1.603106782637256, 1.368768972405181, 1.4010369305800956, 1.5777224855758556, 1.4990160741826069, 1.4832569168503082, 1.5528506368492225, 1.3471208208754597, 1.4204113296319916, 1.3930971144794373, 1.4408150623315903, 1.3492781485577021, 1.5499416366408707, 1.3463996396452276, 1.3662922471759418, 1.5495025562247857, 1.5685320961097857, 1.5416965981515878, 1.2992549433703906, 1.519403312088054, 1.5891477872957491, 1.3253723898897705, 1.552411986149362, 1.5112353172733322, 1.4309767332444483, 1.5162228207011923, 1.3634638732041475, 1.5877334255843678, 1.3908603741895973, 1.440496176184161, 1.4443785660789685, 1.3933504071997502, 1.7237824183242005, 1.5508122292820286, 1.5060396168168577, 1.3172823179700501, 1.3743367835799611, 1.4906658354451425, 1.4507409549909664, 1.3460941226177274, 1.4565512813116954, 1.5082317638534304, 1.4473478801110864, 1.3464197605416528, 1.4790956228563172, 1.4246705471887635, 1.4267189923649501, 1.6930914820715233, 1.4330511061543856, 1.424933266682275, 1.4933031233568952, 1.4400677622817388, 1.2561535120227656, 1.452235635148214, 1.3298525324231614, 1.4316273298614974, 1.4502565610788891, 1.426571061738862, 1.5712789070888136, 1.5593738074562749, 1.5409242759833197, 1.389184950280927, 1.506610010856965, 1.5195593670878884, 1.3011504224043478, 1.2410703784101835, 1.5560495742869447, 1.4382355301401515, 1.495468967569519, 1.4244007753403531, 1.363830037964657, 1.3470954544573035, 1.4993058730037996, 1.3790406446842767, 1.3168924766357106, 1.4136635802634754, 1.2933887188414164, 1.5489537762860066, 1.3775865416369384, 1.5177134657177198, 1.4135513151785057, 1.6189692589456848, 1.565559579946067, 1.745720231582856, 1.4305574873664608, 1.447201347053798, 1.3488519514709387, 1.3121889424082156, 1.6608901992535237, 1.2378648224822608, 1.4705732851452809, 1.7390098413011459, 1.5113425692399942, 1.4289095887123338, 1.3804280600954488, 1.596124199551588, 1.5573616492980564, 1.4762635363605057, 1.3711881076936998, 1.4177963595905796, 1.461754148357946, 1.4709059828044253, 1.428465186530976, 1.4515590988758698, 1.5622941304719011, 1.4021413585595721, 1.4921213476554598, 1.2367864772555301, 1.4856680068448878, 1.4140260753706997, 1.4704766998924552, 1.7572378868415377, 1.464236520845846, 1.4165778980010488, 1.5062697534981981, 1.3573692744129309, 1.4063267269139115, 1.4499089491843842, 1.4854000331471175, 1.459241728118291, 1.4852097021381108, 1.4865652587802642, 1.3824073984075895, 1.555209199451188, 1.3859722131720966, 1.4222612390572411, 1.3515787637383945, 1.3116893418230382, 1.5692055573973978, 1.4416476700932972, 1.4105160840654358, 1.3971004239956004, 1.5604149586939988, 1.5252790543558283, 1.486374551448716, 1.3972254544621432, 1.3568048743288526, 1.5755427051672215, 1.4984019787010758, 1.6776289323539286, 1.5819656759470164, 1.598004750398887, 1.4485961954676854, 1.341132169358911, 1.609905377869207, 1.4798102328140978, 1.512542522713127, 1.8033053563064032, 1.411363313023509, 1.6434981421704142, 1.3920125717937777, 1.5093700307590177, 1.5330715394127354, 1.3859856135781623, 1.2835107046431145, 1.655826318455062, 1.2612060307203046, 1.5754199867873782, 1.5149259887537447, 1.456254560316112, 1.3569893983229422, 1.3598110112405586, 1.42933589783413, 1.5876504703078047, 1.4028543097201236, 1.4755353469293606, 1.4548887821046972, 1.4061514659881207, 1.3745708650150987, 1.486570693588006, 1.523978326221874, 1.3854872198701222, 1.4434590824064977, 1.5137067436818563, 1.54663133381547, 1.4952015053944987, 1.5908387391215695, 1.5522131689461376, 1.399906846431171, 1.3348431696002248, 1.4046651250266944, 1.5262421512701458, 1.5014870483888338, 1.5381521429141185, 1.391747719247312, 1.357243375816315, 1.4279356831917243, 1.3639366375838409, 1.4703690602325998, 1.680913375507826, 1.6654291716807195, 1.4126115418977911, 1.394365527751948, 1.3787447400451465, 1.6333175495810388, 1.5646941945234527, 1.4152126256857105, 1.476374861111362, 1.5689856665738966, 1.5831691974243973, 1.6845960817943302, 1.3829649910956172, 1.5745610383649153, 1.4933410007369192, 1.3563677025410277, 1.418927639657496, 1.3427863086591, 1.4622706194556918, 1.2728810094596164, 1.4382622274265007, 1.2934707387978321, 1.2931188885140328, 1.376848015266598, 1.6830725062037248, 1.2887151466097828, 1.29681298223229, 1.439313440890603, 1.5195939975091053, 1.437371170739724, 1.4127772730883186, 1.514285543551335, 1.4201488623163598, 1.5936700528245484, 1.5917845232439702, 1.3202036608712435, 1.4348438133895085, 1.2691591670448537, 1.520745454003223, 1.429838342838936, 1.1795138571622772, 1.47967203622944, 1.32956109077511, 1.3776537469907917, 1.3304851859474303, 1.4276854392685798, 1.6184844181531535, 1.3492649343065315, 1.4858022676745077, 1.5462817743635116, 1.4567281149717137, 1.4667628875445973, 1.3192176787072558, 1.3398746949566809, 1.432845631355365, 1.374316269184827, 1.6959688173641891, 1.3624175311320093, 1.4101505413642852, 1.4274517262775495, 1.457263421388692, 1.2691375110880763, 1.4541793651501502, 1.4031870140620923, 1.5684514585314526, 1.4167222643879962, 1.3486425908037143, 1.2488237878326245, 1.4675046486240628, 1.5265791584260553, 1.6322688387744453, 1.64345831973443, 1.5747429237474788, 1.4310127726847157, 1.3765301564859065, 1.364384405231827, 1.482595681181144, 1.3559021039666646, 1.408353304613965, 1.5765812672420887, 1.3971324485654848, 1.4468049164916506, 1.4435259825223707, 1.5283408574595574, 1.449081887990379, 1.3213684570380269, 1.4646121813274504, 1.510686508106207, 1.3531832299499758, 1.5935069536547681, 1.6188397382081958, 1.495173405068294, 1.5201826522416426, 1.5568235689401595, 1.4148926897656535, 1.3947124495983882, 1.458318927937114, 1.396494833419085, 1.5056171706004409, 1.5170738706308649, 1.5194237538464939, 1.332606561520937, 1.3670975823522642, 1.4352127622727218, 1.4365883289847636, 1.4808492543538814, 1.3915656564204832, 1.5510505039238263, 1.4783040958711116, 1.2879859943853362, 1.4545437313850749, 1.4294089324671808, 1.3272783055083703, 1.4820386251388797, 1.4850996740564764, 1.615927675774248, 1.417251247465936, 1.4574117166029226, 1.4659245753346153, 1.398056611147108, 1.3893726919834142, 1.3354831640799165, 1.474689194227611, 1.557066476830301, 1.5266772955823225, 1.3724987010416954, 1.5054831747187307, 1.4047074474107708, 1.359708167416176, 1.5970987593295185, 1.474878776177036, 1.4760740894382338, 1.482270134537226, 1.3706996767519315, 1.4373796503901741, 1.453400841538673, 1.5333409081160232, 1.6494962958150856, 1.4470707126617433, 1.4360058083773317, 1.5454168160675072, 1.551636379534318, 1.3929810466853516, 1.4944742826644335, 1.289071992930196, 1.4468593182072318, 1.2633461002389792, 1.50467901940275, 1.3241639193228638, 1.4536708898123702, 1.3083201512677183, 1.4772719936046863, 1.5479175295828547, 1.322820102087409, 1.3645729220906597, 1.5608706859686934, 1.6039489418501245, 1.4326099432541866, 1.5674854912067573, 1.3222099731552288, 1.4746274146309435, 1.391820282198825, 1.42929571125173, 1.5059175209010354, 1.3696988781125328, 1.5571561397787193, 1.4482023816158858, 1.6658845740809949, 1.4623243231306862, 1.6073297327225686, 1.5902219338405401, 1.50582904185192, 1.5595832699118688, 1.5127636652115197, 1.557799149411271, 1.3548317450591254, 1.512336815919622, 1.3422242856418531, 1.5930546446482365, 1.4618118845930212, 1.418928947087778, 1.2131197477084852, 1.4627129606137301, 1.3578095241836206, 1.640521866130115, 1.2883968990249692, 1.3559301648662028, 1.5117093839063744, 1.7232300278378159, 1.5143562165247446, 1.4885168824716075, 1.3734559625227012, 1.3872776447005857, 1.4759327566552187, 1.6651132736283045, 1.5342807419211006, 1.3841410518813493, 1.5159699708423875, 1.6019448757104342, 1.54059896973339, 1.3644851372425055, 1.6610618920359062, 1.4378772333259058, 1.4512512004869647, 1.465491702147124, 1.536364445665832, 1.402465832803735, 1.6146307375977564, 1.401584094787216, 1.3689914777557288, 1.5162125947952196, 1.389514591120627, 1.441878347526779, 1.3886064123794515, 1.2619463596839546, 1.4395560604108697, 1.4442380314212842, 1.6513935089267628, 1.253933662385448, 1.4834109556145392, 1.4314003979582341, 1.3231347544480272, 1.5081277375119577, 1.431678220942907, 1.573727236368602, 1.3562091447177678, 1.2231584038983991, 1.5338752968032892, 1.5251886681025466, 1.430164445789694, 1.3832140426091646, 1.4953935764720279, 1.298414641889533, 1.4278207161390397, 1.4633573155269088, 1.5351462663699542, 1.3695794973355782, 1.4561378499673965, 1.3769337996400546, 1.4901454064246389, 1.3279856200223088, 1.4610967721587098, 1.5996541909400646, 1.4718046413880135, 1.322839604779709, 1.3638921993598165, 1.4324483571400513, 1.3372079520833782, 1.4499958830541404, 1.4962838227941857, 1.5091572329921568, 1.4716028356083735, 1.3970391886368794, 1.6147448181442288, 1.4850919314428754, 1.4838901424793838, 1.4023197170986077, 1.6576882735588787, 1.4024336219904923, 1.6460847443216216, 1.4595423031503791, 1.4892977415385922, 1.4331074353679414, 1.5346770987916631, 1.5091779789302424, 1.415492920642688, 1.4635777259619416, 1.4342015127494172, 1.2859954718699365, 1.4776922252516922, 1.2995778472231692, 1.2913651539288356, 1.5124693140876537, 1.3020258003966392, 1.3626516286617762, 1.402940492207924, 1.400154696310085, 1.4073784366315893, 1.3475996699398423, 1.4068055729909077, 1.4723238216404848, 1.2776851611465196, 1.5420760294864635, 1.495178291044555, 1.5136298426320764, 1.6492544159828983, 1.4186287762093095, 1.3708994727349, 1.549958445735359, 1.4553481332507068, 1.2603695503635075, 1.3892469252925732, 1.3348584282538534, 1.5048377112017326, 1.7108954907971545, 1.4283172350256448, 1.438542732992568, 1.3661833466355369, 1.4094977629150136, 1.5930517354793252, 1.5316720488280966, 1.3227994004837098, 1.5679342818512034, 1.3639917615722148, 1.5827616743677668, 1.6185396731075334, 1.4978581795962607, 1.423688778838619, 1.3888274967124292, 1.5278248099431466, 1.4552079570262775, 1.5133554306859713, 1.5092281188963244, 1.2774526282488528, 1.5463647166026948, 1.452151859662449, 1.4098507912900364, 1.4505108155056192, 1.4413547854903355, 1.651143228679134, 1.5623987128750267, 1.447896926496473, 1.3834721688455094, 1.395821140072626, 1.46514969559262, 1.4351656809897455, 1.3431405754728878, 1.446051956686407, 1.5376158920773657, 1.4489403921588928, 1.5792220122054599, 1.5015384145066586, 1.3241494086974555, 1.352820076820426, 1.438047219926859, 1.5862512409894998, 1.4596503275196133, 1.5084939022707196, 1.3741223399533755, 1.5110438077328012, 1.5273771804241314, 1.4419901515052047, 1.304202520687261, 1.3874316508790412, 1.3978891739396926, 1.4453609456086665, 1.3283114413369412, 1.4933240883822905, 1.2860318982383807, 1.5168007748523313, 1.691090413063285, 1.4587384591895505, 1.4651909380633819, 1.5004272550238569, 1.4164945840613237, 1.2900759228182084, 1.3916159169631226, 1.372344013796864, 1.4305170039122368, 1.292096108193712, 1.3842163604586628, 1.4772704073367608, 1.5789076914176448, 1.3381451897470524, 1.5147495518688188, 1.3269549712748911, 1.546997167575861, 1.4024870460449461, 1.491443766418226, 1.3919980527436717, 1.5780352640534148, 1.5848914383677337, 1.3010320178674084, 1.4954187581925293, 1.482874130404571, 1.35669750788446, 1.4995754468035607, 1.3028993232918027, 1.6994274583046556, 1.4848867703560358, 1.4498666510809235, 1.5208690444998099, 1.477150710825155, 1.450353788693051, 1.4019936091638387, 1.3273690040550823, 1.3963802086376933, 1.3730540238451407, 1.5263481235515401, 1.511479358825898, 1.5034704075090886, 1.5336234697530842, 1.4828274124124095, 1.4129645036677554, 1.475059921258201, 1.3071115979618972, 1.4060773104813646, 1.4911888698568168, 1.2486659150875474, 1.268210009601482, 1.5431736390462343, 1.4069753171891763, 1.42581643404907, 1.4164370631066134, 1.4085994282644856, 1.3106886178052393, 1.6106129365700799, 1.6513645508196033, 1.5228860568830698, 1.2729897691554988, 1.4758831314537357, 1.3178232497961162, 1.2878717234283723, 1.4177761425175648, 1.6053769483676887, 1.4026165122113312, 1.5749460873881247, 1.5290614919536798, 1.3334801345328164, 1.6950589878410016, 1.3866086511129554, 1.3856737163485213, 1.3790231316924713, 1.4344184446400596, 1.468176370874341, 1.195551756468575, 1.4910265600692278, 1.3886302659048098, 1.406170936079233, 1.3341175630191962, 1.4452892918667322, 1.7517448465086334, 1.2579334246073708, 1.499820712615432, 1.4141843001560368, 1.3675112904884603, 1.3811986134077119, 1.500798518933398, 1.3010136076416146, 1.4205596781748133, 1.4131082018974905, 1.3691487329971312, 1.271268463465425, 1.5998072210976275, 1.2246599035237795, 1.5363260200946005, 1.2888124210168512, 1.1612310600913087, 1.6217382894818007, 1.4353142647311905, 1.4001543889464987, 1.4729463670276066, 1.4667492747113031, 1.3316486536182464, 1.6183527272131764, 1.3895442047668691, 1.44215086782572, 1.538603216078368, 1.6276481606733904, 1.3921377908284749, 1.496180273248404, 1.4720798057645679, 1.4899796577957887, 1.3122859780129517, 1.488493939232769, 1.4782028845901782, 1.5249395407849318, 1.3239324559502506, 1.3656403718350518, 1.4372755159281607, 1.60897434542849, 1.6888377445811957, 1.5997212476642684, 1.4555598868158035, 1.285219849594455, 1.4726886107415553, 1.348014905405752, 1.209329303377008, 1.5403267736065032, 1.3441601851396268, 1.4066248719828713, 1.4315226709801927, 1.3704676087816248, 1.4525706632555282, 1.5128996532071863, 1.4370337052914501, 1.465860648570121, 1.2315415968064376, 1.3718600504945724, 1.471342099938574, 1.4621041146312492, 1.4982891266934306, 1.3306717766423382, 1.3234364271065027, 1.3445600085652993, 1.5733183294887612, 1.37646619835563, 1.439235070944196, 1.3505388902223299, 1.551134005152609, 1.4405813704539825, 1.4577123721441583, 1.2514028368344965, 1.3766004546195425, 1.5655243099128027, 1.4397948253554365, 1.4604035413654697, 1.3457388111529096, 1.3113393250865297, 1.3094562747108802, 1.456279796980865, 1.561743238523429, 1.3905343656986386, 1.4808723781419117, 1.3979162282019737, 1.3910839420617078, 1.549793420540613, 1.5451598337809518, 1.4486920303125428, 1.5110704629159681, 1.5001180900928386, 1.467989781126042, 1.386498323680045, 1.4177239198976628, 1.3169996414548226, 1.4743713628304542, 1.5970471313474752, 1.453640393363511, 1.6402468475567094, 1.2573741214824736, 1.3411326565773467, 1.4325797300630767, 1.612899907789824, 1.449604862569881, 1.3409199594107455, 1.2355570650477006, 1.3548772430236071, 1.2902221052054597, 1.3683234305500815, 1.4556848049954554, 1.4713191277835351, 1.4621838162219936, 1.3508818020519604, 1.5443690083705093, 1.4007945421224341, 1.5952856664949564, 1.3907688256933144, 1.4065201988276248, 1.3514526985310171, 1.35741564822033, 1.3879301255578982, 1.1863842444098767, 1.5095859302239034, 1.3437896809817624, 1.5164014964610268, 1.466423421113795, 1.5164938927450577, 1.5477305999028284, 1.303745665002424, 1.3369025884133476, 1.3344346465295631, 1.4001785725315528, 1.3235443645812055, 1.2139479040614316, 1.4861715236726274, 1.5746209463238032, 1.5180528768426393, 1.445970568941941, 1.464932256677383, 1.3840991796120325, 1.5892756909879382, 1.5204519187407357, 1.5922153942447412, 1.3735870890799242, 1.4072063162207729, 1.4474592813858929, 1.4237185401556447, 1.4551568671387867, 1.3045092922628998, 1.4783355812872776, 1.5701367864799962, 1.586771063323682, 1.4416562031787088, 1.3415374732458962, 1.5384790599665152, 1.3532225968002456, 1.3753759332099669, 1.3455540941091093, 1.443218303574758, 1.3192155362026674, 1.3815922634939504, 1.327912554251197, 1.464634381227667, 1.4263023996392115, 1.224410716910387, 1.489470219500722, 1.2578059489787237, 1.407392310033786, 1.5466269742170844, 1.5788523888756842, 1.377339345142508, 1.458838927099548, 1.4966662219428324, 1.5291331801477108, 1.3173626910423026, 1.5138625614731722, 1.3007575850212652, 1.474064184589539, 1.3212036705628418, 1.3615814149423862, 1.5284609876646926, 1.3718334763331297, 1.375283760322649, 1.3483561529804422, 1.504053550119924, 1.5073732318300972, 1.305202124542642, 1.3642078521199028, 1.471337426328965, 1.3650606068145559, 1.5492893233124887, 1.3809917325273846, 1.5876063898427824, 1.6264455048266655, 1.3289032127874838, 1.5072149129754515, 1.5499623402231697, 1.43782431508791, 1.417792068748411, 1.2786770019134852, 1.5190291931358448, 1.394654137945142, 1.4584709073084923, 1.2369442896145075, 1.4695974300639414, 1.3739085107462743, 1.382117669129046, 1.346797355857248, 1.2446339837584421, 1.4789637473501096, 1.4977245413320883, 1.4305662456262305, 1.267549917533917, 1.476755557703969, 1.3078094338282968, 1.39485880507466, 1.430941018905222, 1.3970342958350312, 1.2850021129681173, 1.2934213999989712, 1.4793180684008278, 1.457603734010673, 1.2351654652919182, 1.4062196524606982, 1.5904247535639424, 1.6758788834361795, 1.3127867313704655, 1.4720698332407716, 1.3889952439882556, 1.2311886283668139, 1.4127395598741763, 1.5870395290063968, 1.4378605990041917, 1.520872893206092, 1.4114671076242138, 1.401797597039637, 1.3690910717189246, 1.3488511849186242, 1.324542856025582, 1.3139065051919354, 1.374324252408778, 1.5986109357406093, 1.4567172260276124, 1.5187029510957037, 1.4696287163873203, 1.4333798133369757, 1.2806784402183586, 1.3226323202629675, 1.3901163070299107, 1.4713863893636403, 1.3392452413034703, 1.2654834212577701, 1.3472375873401008, 1.3678556423696093, 1.4678703256602077, 1.3434221980108054, 1.4638398009346545, 1.4340145492973375, 1.6046948180463616, 1.520941085614942, 1.7199709858781984, 1.5051702646085763, 1.4328338976633106, 1.5526372230440406, 1.4814585104377207, 1.2968305804846798, 1.4923176719658624, 1.4734601799106861, 1.498957693205282, 1.2395762992146118, 1.5769518270071656, 1.3535937357721226, 1.379899321801898, 1.5122890829861726, 1.4079146392260842, 1.4345502998676523, 1.3590580662772238, 1.5013898847708407, 1.3028563523297685, 1.46381504481372, 1.4791650732493846, 1.5068992481933143, 1.5867080061629113, 1.301661723024474, 1.5409339561836342, 1.345699169401903, 1.2302408314735276, 1.4387685652848419, 1.5119655631081286, 1.2753788965632489, 1.1555842096122715, 1.4637851335407785, 1.4693202120536863, 1.5462800453078407, 1.4512457065717645, 1.3998689257764252, 1.4753469634190597, 1.4771967820626104, 1.3073224269654704, 1.2515799669175103, 1.3234121488489583, 1.269749979186852, 1.4824895262265203, 1.4789299685180353, 1.3863775244288548, 1.1837831380666883, 1.4559985284600014, 1.3953881508291286, 1.3148984744006884, 1.4071070177857736, 1.47490232364871, 1.476683053077138, 1.4608952155919206, 1.349094748798316, 1.5321902195782082, 1.4788472443173695, 1.4548598795034302, 1.377282183199699, 1.2768377904761703, 1.4711939795297915, 1.3789372200371373, 1.4580309142215429, 1.4549511024658373, 1.4301261585726894, 1.415733621501984, 1.4707346232636207, 1.4962514723863611, 1.4011758954269573, 1.5246404955648745, 1.4976802832554525, 1.3889949113943436, 1.2963914681809336, 1.3003760360129433, 1.4191041520884395, 1.2450636917149305, 1.4172634252028038, 1.4440654458149877, 1.396692157954422, 1.4086299240786486, 1.4765968620699015, 1.3032292243204469, 1.4064373211468046, 1.2914192890385416, 1.5023844748988526, 1.334331656653971, 1.5445768775981528, 1.4925483536369724, 1.4157763878701308, 1.443380248181368, 1.314344591131561, 1.4809431293070154, 1.4545308489586768, 1.3612526982088022, 1.3905770033641722, 1.6161009709641496, 1.3796771746844418, 1.513783027175096, 1.6049131846427505, 1.591653923471427, 1.5569699943466069, 1.4323106294760621, 1.2849937512577951, 1.4182532598197022, 1.4291428818325642, 1.4537780245591534, 1.3526582419492337, 1.5112505803938185, 1.2221221152351118, 1.525081870343157, 1.32168045351277, 1.295925446990453, 1.3871035290173948, 1.4228671977259362, 1.278592681762063, 1.3952102413806993, 1.2835487267024794, 1.604828779615896, 1.5226419488381964, 1.489989130303777, 1.5704698268332158, 1.3642379434338765, 1.3840927131569645, 1.3866419564968444, 1.338799849199055, 1.2678084652997819, 1.431142621692995, 1.33207085003908, 1.3267206530204996, 1.3895293556601076, 1.584414509013808, 1.4064483584054397, 1.3577061271081325, 1.438594770962644, 1.330110857431608, 1.3999029590378385, 1.5378521900743451, 1.3448518660579012, 1.3870230833725705, 1.5339775618004503, 1.3679483335030087, 1.543796286746327, 1.1617197168258202, 1.2786253127312177, 1.393967368142215, 1.4367733535923426, 1.3102153536124876, 1.4041132129103875, 1.2679767732556892, 1.41917846263806, 1.5596026260690832, 1.185326257560203, 1.427611479706471, 1.3928862250412735, 1.2771144455383407, 1.405320989293931, 1.3986359265445119, 1.5053775039794326, 1.3791418040166303, 1.313345823911612, 1.5018051265773658, 1.4018835426861174, 1.3734402060016688, 1.2788429916259203, 1.5430809314928822, 1.3665886505713392, 1.2728515023298255, 1.4733706318314952, 1.4653854904702197, 1.2934887517050466, 1.499462015604813, 1.2788239443556895, 1.375706566407931, 1.3714243606919303, 1.2799548739168953, 1.3385712745469016, 1.4302945071935154, 1.3091290208749067, 1.4643439319984708, 1.4261359055566332, 1.3640502455877195, 1.6458642209168604, 1.4379150962432792, 1.402957952624552, 1.404796815291752, 1.3026046309310566, 1.3824725871467485, 1.4182356810733507, 1.7782705742077136, 1.5489023632865162, 1.3527741944107337, 1.6283031911389139, 1.2789034735709846, 1.3696389758295668, 1.351347000748985, 1.3278229387340708, 1.4186517295141183, 1.4142802216607502, 1.4087782854306306, 1.4546208911125191, 1.4124647430983663, 1.3362072572147696, 1.2277505338403925, 1.4994904142834917, 1.4661423578283368, 1.4132877285128707, 1.4076703834686908, 1.5060391888566025, 1.4641423226089145, 1.4822619437240596, 1.3950091405629612, 1.4042392658260945, 1.3977218632733825, 1.3542715865525086, 1.456962279791952, 1.6388462947599718, 1.4999614356918878, 1.2099952336275355, 1.1016779506106589, 1.3100035765909626, 1.3414448489357076, 1.2315228076148756, 1.351711504291755, 1.3848467331447818, 1.3083421198462615, 1.4530048147158283, 1.4436444242946629, 1.547104314988832, 1.4359378687241502, 1.4764128374636485, 1.3513894091390695, 1.3306018775178443, 1.2438433508623814, 1.3879832802266712, 1.3736862815532649, 1.2173033675812241, 1.4557064766160912, 1.5122054677964123, 1.402434769889252, 1.3726722993769491, 1.4702065881031188, 1.627596829573063, 1.472897147387149, 1.3478927170106245, 1.534149103630466, 1.4653374801286925, 1.4129607054173152, 1.4801034543472646, 1.3908764866187981, 1.4735606113508657, 1.512806221714465, 1.4276467685479375, 1.2508761783738576, 1.2692781428680062, 1.4861347017440685, 1.4703389696695224, 1.5177249573473786, 1.4469556015245448, 1.3053095771242769, 1.370353151626023, 1.3537812306612371, 1.3820134727825752, 1.2618215804930148, 1.4292165581240248, 1.5725184915943038, 1.4030137328838164, 1.2664673537471915, 1.3673787869928662, 1.4907458755311547, 1.2422451318803218, 1.4707270048950773, 1.41347864285351, 1.3749023040980635, 1.483945030798847, 1.3601029004640204, 1.3998886113480582, 1.3452186484808866, 1.450562317423919, 1.320643475767712, 1.3863776138303439, 1.516856579822994, 1.3393013729036334, 1.5494544994433272, 1.3784987509232198, 1.4519862985141694, 1.3868110366575497, 1.5277513863631518, 1.6590022757254552, 1.4228345432105025, 1.516235645231437, 1.3918450030406933, 1.666865318505588, 1.3951642947928646, 1.6528888059276607, 1.2333274263330885, 1.21533903277715, 1.5563037991179016, 1.3599097376926808, 1.4693951926243427, 1.3284516921295095, 1.481554005065758, 1.5732584244419128, 1.2426769050044877, 1.329182707169969, 1.3672220260646686, 1.3924865117695941, 1.4206446591834112, 1.374307411702685, 1.3728306281343121, 1.2191234560874284, 1.3295793209355424, 1.400199194808714, 1.3878380612062857, 1.423537655185398, 1.4348904550700063, 1.5171030179316158, 1.3405799858025662, 1.6085228444759299, 1.3780495065672915, 1.3925846776281006, 1.3026655463393657, 1.2653815768131829, 1.4214457675087442, 1.3603365246763506, 1.3647479968344003, 1.470993800195833, 1.2936292974801233, 1.331972299878184, 1.500402164950848, 1.3667433969333522, 1.3757948189580282, 1.3909887812253587, 1.3657882303341622, 1.3482625118176335, 1.3779960771499054, 1.4383875847312664, 1.2806870366321672, 1.6518594471856105, 1.4594250582668835, 1.3775971348984706, 1.6120324929936225, 1.4116189897582052, 1.3931011759343574, 1.281507896939633, 1.3734158493324602, 1.4323620677278672, 1.2308128327536167, 1.4580212399905657, 1.3321202552991638, 1.2513547596316537, 1.3876477839061432, 1.4929013931805581, 1.4203875966246013, 1.5613779952308464, 1.4516185629830158, 1.3429549303154484, 1.3214746416354117, 1.4032805417173015, 1.4925331052573798, 1.293184249783654, 1.3569365460279113, 1.366715749729795, 1.3726839480471194, 1.4380088033376794, 1.4700119744773343, 1.181252175570632, 1.2824990369295932, 1.3992741643696536, 1.3370978033482237, 1.2082156419504273, 1.6110368423418882, 1.3442879123122944, 1.3538446772893042, 1.4689686447190013, 1.271583328028479, 1.3410551748674535, 1.3669376377754574, 1.3868918102673182, 1.3328331034281806, 1.2664596968154636, 1.4476040638808565, 1.4483092342528459, 1.2358165111119228, 1.339835199055142, 1.3136907673674398, 1.346132967580525, 1.370936112901511, 1.6287995158210282, 1.3472226113502537, 1.427538020531444, 1.3643798554852022, 1.4071127795444667, 1.4160356762562223, 1.5324183582976099, 1.609037853044316, 1.436813574014256, 1.4065988188986327, 1.4464011863912072, 1.4891680983629343, 1.4346580356320047, 1.3733331942783906, 1.3235030728766624, 1.3139499227616076, 1.6120671677938652, 1.4023767532468698, 1.278450373933202, 1.357158503232748, 1.4976521814838648, 1.5158624937742613, 1.3286311966826319, 1.414083617452836, 1.371229740224986, 1.2110344614593034, 1.442297574096243, 1.3398477724781863, 1.2828580254895103, 1.3060146772918557, 1.200355073496501, 1.313497098782451, 1.5014694177899142, 1.5605365725594753, 1.3454005190567253, 1.7162415582539643, 1.6007665901449384, 1.4687051422213349, 1.3056671932546946, 1.440582989361846, 1.4735053697314668, 1.2436577389748067, 1.490331102009698, 1.4618003767625263, 1.498300828135325, 1.3662199049038983, 1.160763476809201, 1.3227630091711333, 1.3203958196312013, 1.3854846916991588, 1.3299300311030442, 1.2578826771765537, 1.2922161114234225, 1.4251294684164009, 1.1864554603826543, 1.3686163879990731, 1.249304646227432, 1.524695876031348, 1.328124429003179, 1.2743106181671915, 1.5387418299320246, 1.4352244262568015, 1.4087714478677003, 1.4348584169451568, 1.454265313142916, 1.39344033933007, 1.4125320995504498, 1.5961407394250906, 1.4307001355669688, 1.4454269883036381, 1.2804972174280735, 1.2820493503506107, 1.4558308491056968, 1.4070011206284432, 1.5421046180202973, 1.2758996188079585, 1.5113563294723351, 1.2700420738264524, 1.348619671167581, 1.3330076709354994, 1.3841911470230988, 1.2008128124869235, 1.6065790467949241, 1.4667537930573165, 1.4180695086547974, 1.4042440222990533, 1.4562368387980356, 1.4180819999478225, 1.5202048249358726, 1.3992013581027125, 1.514657938695308, 1.5335229774937542, 1.2388759217378895, 1.363954283787744, 1.408743162438599, 1.372147712669778, 1.2994920760356106, 1.289883187623342, 1.4305929319293043, 1.3805754143455973, 1.2828628832745794, 1.3379795148926439, 1.294793706022706, 1.4425233571839242, 1.470737434148453, 1.3674245584924865, 1.3548326323308664, 1.3068411777958437, 1.3551303130536703, 1.376005370528268, 1.305626868902004, 1.3568277890033695, 1.3778144977389788, 1.4147796836611588, 1.4398104322489158, 1.327857711621614, 1.3901578305696154, 1.2662197326657172, 1.3181055650762863, 1.4104614348402984, 1.3848913286977522, 1.7366707395691001, 1.395107891351405, 1.4598068822927892, 1.3215675831846778, 1.6097159986372533, 1.4335150165617216, 1.3166126962722464, 1.4065011561283105, 1.3927737317101878, 1.3399416785356422, 1.3980622491688732, 1.5261686194639967, 1.468649248477746, 1.3075310650259901, 1.406190958317481, 1.3853196532529746, 1.4378386961782996, 1.530712070144335, 1.277068367130246, 1.2263742001073539, 1.4675330577618124, 1.5073005313575623, 1.310333651352194, 1.46500245146618, 1.5671052497427038, 1.3726277909851172, 1.265608290177529, 1.603656569368721, 1.3952585479531805, 1.2713032789173213, 1.256279926384214, 1.2865794112630438, 1.2498809166042275, 1.3976612439670597, 1.2157083246175777, 1.3888746380285206, 1.4070217697685872, 1.297089846946439, 1.1662023588852668, 1.4564128824023173, 1.3103735791022073, 1.3624155118908403, 1.5397095270159646, 1.3992389469110176, 1.3982745705219386, 1.5428535680542617, 1.391409165351257, 1.47072543306926, 1.4629353376892693, 1.5913725539386876, 1.384318593794435, 1.410040328753311, 1.2922851231416381, 1.2704036940880012, 1.4171979573003564, 1.4851056619430565, 1.3222667993336725, 1.3441337717466149, 1.4016926360460331, 1.2952475819588622, 1.3352671968110537, 1.5530640088489975, 1.268599123804146, 1.5432638830400884, 1.3899321889365819, 1.4691645633022683, 1.2907894966731535, 1.4066294440101093, 1.283916947732419, 1.3648221955991802, 1.6605532608071047, 1.305457136817799, 1.562626413926026, 1.4073291586139172, 1.2649241367539343, 1.3251589998600695, 1.2941519940360846, 1.4158159246055357, 1.3060150618401363, 1.2966632040523836, 1.4160756522626687, 1.2507304231194771, 1.2692057062648734, 1.4235647985995792, 1.4356533105022717, 1.3603301105394914, 1.3605822236886573, 1.4261477422998456, 1.4954054681271949, 1.4315970471899166, 1.241229527855533, 1.3107532565005175, 1.4449822228490894, 1.4543376417123224, 1.313953950348567, 1.2512054082327695, 1.3597610316521902, 1.401233802890702, 1.249991544594745, 1.4657093107312427, 1.5756045415713495, 1.5452344900257236, 1.4854734934556217, 1.4018045529161598, 1.4730770298309923, 1.292629663294924, 1.3068053084649183, 1.5444590352148353, 1.4513145538392127, 1.316070356619352, 1.3594569864165704, 1.4630136183381872, 1.319865508215974, 1.4221083869089377, 1.315284315631551, 1.2931070201323789, 1.3059027472246196, 1.5649459637683938, 1.527277028991919, 1.320161918525566, 1.4513139960494714, 1.3484422692918716, 1.183690682055694, 1.450511572677032, 1.4379824911438683, 1.5961724379902855, 1.482569666598583, 1.3656571167161276, 1.3506347978108366, 1.459740053122619, 1.2927855301469726, 1.3757219423973417, 1.211637784342694, 1.423047842266427, 1.5957441001490276, 1.3833852048403796, 1.289013045628781, 1.4550085692820878, 1.3568665480011797, 1.3894483701505191, 1.3030440583681517, 1.362852400864257, 1.3579450956780967, 1.366821568060171, 1.4461067189014931, 1.503649715806036, 1.447672244971322, 1.4508505900779907, 1.5542757918115455, 1.4329906763374505, 1.6781373779639654, 1.329103834664408, 1.4889665852302545, 1.2999884976641416, 1.4824841876326622, 1.3927112612531203, 1.6087450824812028, 1.4133795399073341, 1.3347242405289999, 1.4615194463803352, 1.380342702810889, 1.337539759965365, 1.3265381597893464, 1.2784494747755912, 1.1591769915081487, 1.3881517740736544, 1.2706026069175165, 1.3285006927758305, 1.3830893541584328, 1.2802455127801184, 1.3351977872931398, 1.5185695327330746, 1.4912177355730056, 1.325333973157834, 1.280706214336951, 1.3790445753860123, 1.2763717571869109, 1.490774426453615, 1.3956773056020981, 1.351817695654365, 1.5465548502414035, 1.5991064712651692, 1.396046423802258, 1.417579235660813, 1.3888744583311747, 1.3945893624021601, 1.3573797170093962, 1.506419663400882, 1.5261947865670453, 1.22294745489798, 1.3981366880163548, 1.3741117645565433, 1.2818876916788857, 1.3112537402055708, 1.3382419846477547, 1.2746439500047408, 1.6545609999100988, 1.3725167154905826, 1.3702569465520495, 1.5239545182315328, 1.2655064364701567, 1.3098221422124907, 1.3021897672671074, 1.471066173809754, 1.3302820121460137, 1.5445630832706734, 1.4798081602307314, 1.4202654049332528, 1.3596707950659888, 1.2903586829831997, 1.3147886990395399, 1.3179090056804454, 1.4658401984204232, 1.3237113575089279, 1.6765730295606307, 1.3222675413028477, 1.491313644661073, 1.2284503198390224, 1.4357123567114163, 1.4251545725426047, 1.3543664930100163, 1.4957632628943438, 1.530899939990218, 1.4952229002105286, 1.2267383990100753, 1.4249987141576483, 1.3378723114103743, 1.2575189671690143, 1.3540022219312504, 1.348080822451437, 1.5269945606560538, 1.6149618124912521, 1.4623150902618272, 1.6191967941455467, 1.264047788554043, 1.125308044192838, 1.4451200265080286, 1.1746718951107664, 1.5136524197968393, 1.5117574188272764, 1.31848022186228, 1.3096785118586434, 1.2861183471130184, 1.2712515586967827, 1.3638792939701256, 1.1531526434865933, 1.2526914586148634, 1.257441102922752, 1.5154566864822052, 1.3149941491349537, 1.3027803329407501, 1.434586643392479, 1.484218269482537, 1.3680189626143835, 1.3140089226871254, 1.3561180953300351, 1.5695079513712837, 1.3378927600651098, 1.5531669406669826, 1.4516741862959777, 1.4776582772206128, 1.4836422077340463, 1.353104537427641, 1.447418590070012, 1.4013703400550712, 1.2998556948428532, 1.4462168844557746, 1.4026699249688677, 1.376627228587721, 1.267954248748638, 1.230071409077485, 1.4077915201842688, 1.302252640179539, 1.2287065979181377, 1.5355425897129575, 1.5336724014308925, 1.2703670290766897, 1.4819493867329763, 1.3546019489759582, 1.3665191795505833, 1.2869762713013209, 1.4464043301683949, 1.327935994399825, 1.3843237462398852, 1.2551263592753057, 1.419681232745512, 1.4018543542884698, 1.3581878497280497, 1.514582208456337, 1.5181491647487666, 1.4704775583139937, 1.183048120891524, 1.3469121339289265, 1.2177788894916461, 1.1159340985846133, 1.1790594988956695, 1.4805987573635147, 1.292449780504112, 1.4311320989244192, 1.3605165938553896, 1.43442010213594, 1.3554728498635251, 1.2970504119695194, 1.2537351814219346, 1.3774614594813215, 1.315138465339196, 1.1043504225830585, 1.3078133764803936, 1.3696722710817242, 1.3665694438036948, 1.4488472435271458, 1.3525246529239854, 1.3693705743162485, 1.584625572180208, 1.7043575329615124, 1.2330264968723557, 1.3499794957406162, 1.417929660058189, 1.3706119348863015, 1.433192122877903, 1.3906393624951279, 1.6216867581467356, 1.1663025173565145, 1.3400661050364848, 1.4402688403445894, 1.5860532507696274, 1.2975031460931132, 1.4345325787910863, 1.3380436001897094, 1.3279764385823984, 1.3571305484336733, 1.3981324135134543, 1.2989341594551447, 1.1869906978588913, 1.344612404982973, 1.3541716789188896, 1.4828140897956732, 1.4504870122563487, 1.4958274022233877, 1.355751765064051, 1.2454642295812366, 1.2307530293810478, 1.445217012530322, 1.3499952289110673, 1.3764166735099834, 1.1005564364676401, 1.4852036937196376, 1.3313862304695834, 1.330303089136456, 1.5487759471602045, 1.3392591442799366, 1.3108527611965999, 1.3998751388370576, 1.3484715291964224, 1.3613464424959187, 1.2064478753966843, 1.5310526742063595, 1.2755012373005414, 1.2961263112078922, 1.2630004644196255, 1.4529196833609768, 1.4057977396979668, 1.4325531121829647, 1.4238379492953726, 1.4726625762488645, 1.389730456341234, 1.4032606203860456, 1.44914259545069, 1.126388059934507, 1.4409020956460565, 1.6071290577080288, 1.3822425344912808, 1.3069593975848242, 1.5828718544791094, 1.2765103733690677, 1.3982142700571258, 1.4047403304915966, 1.2800646346969584, 1.2870316739166285, 1.4142123407074962, 1.3643284569040701, 1.5675121548804096, 1.329982011329736, 1.3295111570002445, 1.1825610155226471, 1.472114861889926, 1.2390878976381803, 1.4411746398478567, 1.4225902379529478, 1.4041213933108185, 1.2520195177672173, 1.3860745939624552, 1.2317552495959014, 1.5984938385722876, 1.4275138672362457, 1.327196665130795, 1.4466273194285646, 1.3605049926760844, 1.1918225368019901, 1.1732140124259072, 1.4830999469785768, 1.4777742609739772, 1.193322641190202, 1.3520069309404852, 1.556891271850625, 1.4900504104997916, 1.3335121540856099, 1.2448677124660366, 1.318169436843192, 1.3096720718539452, 1.396014211543825, 1.5339838087264557, 1.5303926775377195, 1.2976466951404517, 1.3723981034400072, 1.4257547644466344, 1.349626857389156, 1.2691593559703993, 1.2719949054278576, 1.3193673036211158, 1.568105259186783, 1.5411655497913137, 1.4813068206211475, 1.1670937576138947, 1.319348943499061, 1.4968442340054378, 1.2893559934383148, 1.4076790294566175, 1.239616675324113, 1.3967677194028079, 1.5020776501083026, 1.4365318890041372, 1.4058996805787882, 1.3593091644863755, 1.4018963996435272, 1.2199372019202617, 1.2641864761227077, 1.2894414827888026, 1.4779577707533162, 1.2992480468506258, 1.215976648587692, 1.3006558884320356, 1.3044001106955196, 1.5917052188502765, 1.5387450031930319, 1.3861079740563758, 1.267235991429243, 1.4008432850457084, 1.3954089897711566, 1.57685458353646, 1.265217282072606, 1.438532393278047, 1.270535398112726, 1.4875091932789963, 1.4727636433798668, 1.4846401012982506, 1.4888750952557643, 1.2924011086363225, 1.5769269111604478, 1.4542872551975277, 1.2144521787949438, 1.4198076146106517, 1.2937669270093968, 1.4690977118066564, 1.1774479891884708, 1.3589911198637543, 1.456488407176287, 1.2061624035123832, 1.3685400657094908, 1.4301617819340384, 1.274745684814026, 1.2637544061784367, 1.561462088269951, 1.4774352849648247, 1.6226806835106096, 1.3648539960898947, 1.577888585051834, 1.2966515371562295, 1.5016314727202407, 1.4408640911527169, 1.5001015717306212, 1.365258257493268, 1.3696643660078012, 1.4786837634409622, 1.2933673762153888, 1.2715312444221993, 1.1263596515814773, 1.422487988556268, 1.1732984831808517, 1.1831211387778604, 1.3847257140458185, 1.2791926991499558, 1.3422782466874141, 1.4320626776878316, 1.4519754470659674, 1.4149719666265048, 1.3882393802874724, 1.2753783233696487, 1.2995663187671818, 1.6365474308355261, 1.2784800118484825, 1.3650391448564108, 1.0942986705141813, 1.385822233674993, 1.622578960647319, 1.2313775215404168, 1.451441333318314, 1.3180387486243856, 1.4903600895084765, 1.5371854776279887, 1.4581136605287088, 1.4688834170332121, 1.3192215130143934, 1.3980538746214262, 1.2241768661456258, 1.367609135495715, 1.572851587313477, 1.432382014538279, 1.4243143506464295, 1.532937497774001, 1.3812051047404241, 1.421046477132156, 1.4455934367726238, 1.384354109543716, 1.6157708161029136, 1.470551367672688, 1.3402644894723437, 1.4530190853176512, 1.3874835100296548, 1.4762839608790812, 1.2998447118620253, 1.285589450203036, 1.262150271546085, 1.4010580900446898, 1.4279502822127872, 1.3097823682494791, 1.580868480037556, 1.3525928442086033, 1.2686937840108659, 1.430618502588214, 1.3117995699419103, 1.4175027281541857, 1.4998377036212165, 1.609382220738268, 1.4446886766837432, 1.4864485823446105, 1.3573318949933013, 1.2657637932559374, 1.4771130389718996, 1.2685179409934138, 1.5224058078976537, 1.2114380810916163, 1.4700569199529014, 1.4784321584636948, 1.3918907630043824, 1.3087626652865714, 1.3458428775180198, 1.4483881382190913, 1.4541561333657882, 1.3376559371088548, 1.520191721120962, 1.3825692317291898, 1.3313997115122722, 1.2288734122928837, 1.239902345065842, 1.3733597339355876, 1.4616104686899232, 1.4942285214188493, 1.3712304108177196, 1.380271814699739, 1.300205269990704, 1.6927884684451044, 1.3820730201796363, 1.4137110536761623, 1.400026140898833, 1.4256353446070889, 1.3176377975589741, 1.1896350405649292, 1.2592185129034517, 1.302754413169936, 1.4741311503634325, 1.5601892248745477, 1.3505193861645406, 1.3699168588202824, 1.4792987693402835, 1.3637694452635245, 1.383363539064535, 1.4670402296708906, 1.2097762816320317, 1.314053731079961, 1.450786497503014, 1.4279337879204024, 1.2975116561167375, 1.3688474497375231, 1.2383270642380375, 1.424043225353143, 1.4864552122168981, 1.3859249788232058, 1.4403815597882597, 1.5026835961606755, 1.5621982015957605, 1.4947104726885119, 1.4275959940386127, 1.2992577474072948, 1.3931729858949062, 1.4148783632884459, 1.2570746297429711, 1.5268967150122847, 1.3357813045299585, 1.2418964953519174, 1.3447592221366296, 1.3164684600697436, 1.2715940504180656, 1.3669081771402631, 1.1861350470203265, 1.3560564190452886, 1.3514810095479035, 1.3956710696313406, 1.2482742534789038, 1.3541209220266555, 1.339793734188766, 1.398362663056907, 1.3768802088009278, 1.395169191714543, 1.3675383019624947, 1.2943986197391075, 1.3607042201830126, 1.1970086038093926, 1.46220305341818, 1.5363462310164102, 1.2393645505300364, 1.3669983040872622, 1.2704242459009971, 1.5263812243843953, 1.1345302145272518, 1.352305658672538, 1.2608998394059565, 1.2393851886249336, 1.19639099751017, 1.287422341634248, 1.4555726467756118, 1.4195296056724451, 1.1966692102390608, 1.1446388257685445, 1.2389921244588435, 1.3931958718859954, 1.368240936013463, 1.2587602107815055, 1.2550512092822077, 1.706344914723051, 1.3878577599840944, 1.3891982834762024, 1.3960473583159392, 1.4018392378126552, 1.413584801823309, 1.2984855641818378, 1.3390249334585262, 1.4309839007851732, 1.3052926434702927, 1.4780251954788468, 1.4602866951052922, 1.3558553405697027, 1.2712826344270503, 1.2166375173074278, 1.3668492953148461, 1.279284445081182, 1.436316410751975, 1.4081851296789158, 1.3505834809749913, 1.3224778291456605, 1.427473779724317, 1.464133998220303, 1.270644382610436, 1.4678066496400257, 1.377111890701579, 1.4518716251177037, 1.3317539283011945, 1.366451436851864, 1.3729572471956173, 1.315986368046444, 1.3954158076582837, 1.4660140440138973, 1.5048340861702432, 1.3386677037909467, 1.2872156280948988, 1.676908137313448, 1.5338812852144068, 1.593705497023904, 1.3376890828680161, 1.0788051875377023, 1.477292693580022, 1.1926715007124407, 1.409224139692376, 1.2681514258968205, 1.3813607090103168, 1.3636269944727397, 1.4276936904036805, 1.5108435277559988, 1.4103978497675587, 1.337438645929661, 1.35731626397453, 1.5365517170365117, 1.339431668451071, 1.4986263827087265, 1.2723928421480957, 1.2892945631327164, 1.1085051337057004, 1.1850534944129818, 1.355587794012716, 1.4851860884405743, 1.3875267307483004, 1.1120238569840857, 1.3118950832058704, 1.4233378094969922, 1.3885571354583346, 1.4776125296600648, 1.339415459644618, 1.589276804570106, 1.3163040347154549, 1.285607710488161, 1.3370636188958847, 1.2634216104063345, 1.4254169629612554, 1.3213762409252905, 1.4094039032323957, 1.2604124450981795, 1.1515748142922428, 1.1355889129230234, 1.4039276862562104, 1.4569352477919504, 1.4366966209132286, 1.329930812806985, 1.3953925256018531, 1.4367855425401417, 1.473226196299878, 1.4989595390720143, 1.3299703323953653, 1.3894100781932577, 1.4056563279281542, 1.3442518878520877, 1.335911254321666, 1.3574422643307313, 1.2911797049301668, 1.236358916813086, 1.4615982459070442, 1.3741187133950328, 1.4157809386075888, 1.4991680915214167, 1.3410337582440022, 1.3911138781988905, 1.59115766115897, 1.4283665598578816, 1.5806926448170222, 1.485010936888533, 1.465419244793573, 1.269797753902092, 1.3041111648659116, 1.2430433351984858, 1.5035269013999828, 1.3422450708190383, 1.3716318611174347, 1.2470402854963665, 1.170479717220384, 1.6429210726063792, 1.4340765840169931, 1.4798451795349978, 1.2196249959942114, 1.441041547015404, 1.3094937149726291, 1.2777095268539476, 1.305350202704922, 1.2948554508035315, 1.1691651155318175, 1.691910109891616, 1.217878435443292, 1.3186424547398012, 1.2895704705596567, 1.244324667175798, 1.4289378782814885, 1.2423926926664457, 1.2449648153975996, 1.5201574393509447, 1.2978119105320731, 1.378678914130274, 1.2427714996200872, 1.4252256175898355, 1.3027041716281302, 1.4145891921558342, 1.2801916351460125, 1.3865638062310166, 1.4400924039442053, 1.3552130555359683, 1.3968536359778954, 1.2320840178525314, 1.2176969757378067, 1.2931772542589868, 1.5686665190668534, 1.2443003305897458, 1.4326971452493427, 1.1971294875400484, 1.2649012866680054, 1.4405855828850402, 1.217914626261983, 1.364346902821805, 1.3002424509332582, 1.3169340647607353, 1.3129903432809218, 1.2711416775784663, 1.3878046611709893, 1.3434346957484267, 1.3718470985585882, 1.4519115049297977, 1.346868407629219, 1.42896112176909, 1.4381086415408475, 1.2751230665396118, 1.2092116573931577, 1.4857637641690358, 1.4127323655419628, 1.3886315321028244, 1.5892162674230335, 1.2320000136447868, 1.4265665014196938, 1.4803358489565783, 1.3030097832151661, 1.3904352492652763, 1.5055739041540956, 1.3034437624632351, 1.3905319483810723, 1.4891710467343606, 1.520600508824945, 1.6962513253360043, 1.3896927551192002, 1.3386924519326484, 1.430041475747382, 1.2264874679876165, 1.4252813746598063, 1.3328467445747603, 1.2990529657238676, 1.2727261669715344, 1.3646554635851798, 1.4477742109568528, 1.3292986502696875, 1.4394622822556866, 1.3354971588230606, 1.366984413117469, 1.546010318524067, 1.4466236052653891, 1.326664197832293, 1.27884381050712, 1.3724980415982984, 1.4319311496857166, 1.2991924801536552, 1.3771492428655177, 1.2821477835042765, 1.464161678810028, 1.3218452066222233, 1.5479306361386072, 1.5299362475709197, 1.414556771152059, 1.2412689283923344, 1.2956671720105484, 1.1376382156804619, 1.2242368453730208, 1.1818472260788229, 1.3495211933958036, 1.4506563575663476, 1.1458534052374845, 1.3420036976331144, 1.2950680990809516, 1.3761330413050594, 1.2001940790101262, 1.4847283160349525, 1.4720978411169485, 1.3898596987760015, 1.519865250253099, 1.298660528628067, 1.3058422478174427, 1.238724779224978, 1.1908514308335476, 1.2555201086413956, 1.4183292863098855, 1.5420006646819888, 1.328860561224443, 1.2795036671746325, 1.4027493556572366, 1.065999401499688, 1.1694521252774635, 1.2508366090604535, 1.2256646717751065, 1.40932937728598, 1.4935656840015694, 1.3906786366531625, 1.2762343089044546, 1.364590607378446, 1.169313074652858, 1.1772431707419762, 1.4403436473003213, 1.2722953814758393, 1.421035932938464, 1.14847898735076, 1.3023185726981539, 1.444079607440985, 1.207322462613356, 1.431560222213461, 1.3590541906153661, 1.262977383198258, 1.3383823598969413, 1.346768421197472, 1.434182811985832, 1.2227992272945176, 1.4465846078286584, 1.1891508843794223, 1.3860033656188622, 1.3654607906802656, 1.320168234454202, 1.1631485719445223, 1.367488253027509, 1.2338831723481705, 1.4119477218071739, 1.2461580180746836, 1.4389406167726628, 1.1885659140712874, 1.2998040011595262, 1.2198071596310245, 1.3852678286753757, 1.3606499339714344, 1.3263632135090242, 1.5112454003717144, 1.428747570737652, 1.2061930612964125, 1.4776509415846604, 1.358880090024618, 1.5700260072070122, 1.3562983731723286, 1.3139737822364004, 1.3425855875047226, 1.4847581310038873, 1.2344965266935828, 1.3330672380982915, 1.188564799695618, 1.3836720433036682, 1.4306972211283135, 1.423287056216445, 1.3258939139591623, 1.5331917168614475, 1.4272469287959109, 1.2228861878589594, 1.280601892340209, 1.1673663084910495, 1.2866468229471977, 1.264794761712137, 1.542734368458126, 1.345399807461091, 1.3391336376091931, 1.3922041874320852, 1.3878411807606883, 1.2249077870996445, 1.0525291229337899, 1.3242313194380353, 1.4056314133557593, 1.2283189138358086, 1.3466255536632383, 1.2920103543433032, 1.3887124329497447, 1.417753108567969, 1.3643430955609566, 1.3126179236892463, 1.5327753593333875, 1.5577359837972928, 1.191096981360638, 1.3259015471239142, 1.360718335472341, 1.3039815273960516, 1.3596640637326727, 1.229154823428641, 1.2742208497168124, 1.4361445357422755, 1.2997779819740134, 1.4446022930665392, 1.3124931995825426, 1.5696410531661757, 1.4151303065281067, 1.483790407581375, 1.2163805301962014, 1.510065678735938, 1.3991999744099788, 1.2945251648589118, 1.2911489411406232, 1.2763187712723951, 1.4527079235640283, 1.3113563152076473, 1.4324125253141327, 1.5419156592636887, 1.3492496051638068, 1.335258366570385, 1.4402808449966777, 1.4579515489374983, 1.1891779159241138, 1.4290517417882445, 1.332818648105896, 1.4022487865649782, 1.1513829638661255, 1.28529773663716, 1.2994111423947663, 1.257085035228039, 1.1964974676984632, 1.2200069393944561, 1.1652473252286693, 1.3532759148591407, 1.4828714563948304, 1.4521160046343238, 1.3608011726822278, 1.117392990825736, 1.5197479221725105, 1.5213339142210671, 1.3161311609253297, 1.310392638170932, 1.1100293996156672, 1.3442564043901655, 1.3869911865725277, 1.2158339379956618, 1.5550622918664865, 1.5048851331895798, 1.38340439272119, 1.3218141191764565, 1.3612815319572482, 1.6824854418762316, 1.2927396614269713, 1.2738016944639257, 1.3546936458017842, 1.4287193661677022, 1.2625198302038527, 1.4899332297388204, 1.552336901277547, 1.4540145824232809, 1.3638675335229344, 1.3418881011849026, 1.3193374505524664, 1.5559403881503147, 1.4774164154663663, 1.3217442746838541, 1.2643125253931669, 1.4314648864199517, 1.3997224443744134, 1.4704803016377397, 1.2396447933822676, 1.4073942889028022, 1.3801181757613432, 1.3690616752958378, 1.2765954284030625, 1.437873097689169, 1.5369934419492806, 1.2580805798368357, 1.5404838322183638, 1.2482355170612092, 1.2347362716910038, 1.3408408144966166, 1.5222683948394247, 1.237428051193349, 1.4091294879751894, 1.3734339486540341, 1.5517479635462965, 1.2235966592640193, 1.4487652810283629, 1.1578927740038383, 1.3692130230533177, 1.2221694469562592, 1.3474108781299683, 1.3659125256595155, 1.4247717670005184, 1.3716145969659337, 1.310218124755217, 1.533103216773418, 1.3189833205108652, 1.41385303786288, 1.316302051603056, 1.3950042116271402, 1.3051985886629196, 1.3378373049123014, 1.3954590633623272, 1.6272832424888892, 1.3347089622748587, 1.5103279027949308, 1.2950379784039208, 1.3723703332947184, 1.5036238791653291, 1.299931800028574, 1.384158024129635, 1.257740204470217, 1.4627230995029898, 1.2933326121027013, 1.4075180456201988, 1.260775253843739, 1.4022811781283258, 1.4102252145808447, 1.4401743202561084, 1.704164064483091, 1.4900623743571546, 1.4201351584662725, 1.542314145058335, 1.3603008834470662, 1.2063363536136071, 1.2875079030917431, 1.4523764993761097, 1.265730502840611, 1.1766759830652924, 1.4384996823963394, 1.209127414506069, 1.3797933609414048, 1.2712488355801812, 1.2235724415178015, 1.3992521323557168, 1.3290152038484286, 1.3145089729307113, 1.33879013550577, 1.338957957362335, 1.2191668188672358, 1.4852351110027635, 1.3131752539451846, 1.5802328097754204, 1.166117819134068, 1.2707273875118723, 1.421873660736683, 1.5612891346205908, 1.2958420018315129, 1.3625215819537357, 1.4807885568442296, 1.5122215414513491, 1.396180867261699, 1.2989064214259372, 1.316845523814038, 1.5274480190807282, 1.4033841378425225, 1.399791990033755, 1.2409204664647329, 1.3935388612445365, 1.1783149946421658, 1.3000582640018357, 1.323492666179816, 1.2826946317078722, 1.5616722273587187, 1.2535062068650271, 1.2655396334927431, 1.5301969180245079, 1.3087618041308346, 1.387292614805251, 1.3584913099762614, 1.3574227535418202, 1.248882164137321, 1.5466577531913575, 1.3148464845987777, 1.352420634161447, 1.4240972910450265, 1.3829967899612194, 1.1942688752769164, 1.3935794543610691, 1.5277171612845926, 1.2947539743974412, 1.251451667538761, 1.5379907305264402, 1.1302509836942158, 1.4322383534319558, 1.2384535691680219, 1.6261270743031218, 1.259155267925097, 1.3628536139419125, 1.3971819888805816, 1.3073368889691679, 1.187006865378558, 1.2889873947970991, 1.1040529090396651, 1.2398894895820909, 1.2099380742123798, 1.3982077250066924, 1.3926680282766282, 1.599525974429819, 1.4844583083315839, 1.2567720247510628, 1.4935018002765066, 1.2829954021759735, 1.276595143877436, 1.705617545622374, 1.3162640910441759, 1.5172461778212978, 1.2477887010345332, 1.3819870930803717, 1.2842828697011186, 1.2340300863183453, 1.1658677489805898, 1.305668520108125, 1.196937306483534, 1.410787822628382, 1.4504824568231889, 1.2364717654026536, 1.3238228119452196, 1.3514524991187702, 1.4069622030366806, 1.5610220498511762, 1.2400072372581175, 1.319150781359978, 1.3071147679106945, 1.21842556287411, 1.393556945746652, 1.2480291425279912, 1.3938721701537415, 1.3267075633010919, 1.181054534562489, 1.4914700858425878, 1.3511998023140266, 1.284378269501091, 1.2153185822244823, 1.2237735906670464, 1.261015405562178, 1.2407664071785065, 1.2650475230467575, 1.3236374473544004, 1.4113449054431912, 1.6048696311308748, 1.1556039262810407, 1.313071370476679, 1.5145455215938182, 1.2587193318537249, 1.3346964503228613, 1.546710330339204, 1.468170106624802, 1.353402169913883, 1.4005356315814357, 1.6027953239473876, 1.3434352098527866, 1.2758308505650564, 1.1671382557639942, 1.4419083948927796, 1.3274874852455383, 1.1927171697153836, 1.2695914077758044, 1.324050042897867, 1.2884422159270852, 1.4766942737883, 1.4223538415750179, 1.304227029173874, 1.3491500872052031, 1.4734980956090131, 1.3657597618704094, 1.3636508747068783, 1.2113712033159492, 1.318510311236671, 1.0680231915950222, 1.226305608095104, 1.2167423394852301, 1.4591550142399723, 1.2645529399903015, 1.396625373932559, 1.1438199478249025, 1.3892433887912414, 1.2862796297472452, 1.2755449447061027, 1.2928304374151793, 1.4452324552631166, 1.354322027968434, 1.1329490466551453, 1.2587140744877479, 1.2456761278009618, 1.2258475231195727, 1.361604139284272, 1.2373805534914455, 1.4312893314523887, 1.2993716261237958, 1.208788093709426, 1.2232768418028168, 1.409759544699079, 1.096147021671532, 1.1903473754603628, 1.3119898270978334, 1.2922168836664536, 1.4279341542135244, 1.1530844770721485, 1.4319972623878714, 1.465250462937022, 1.2506925515001368, 1.376517943778744, 1.3158639467281252, 1.5722629097064567, 1.3664243194265981, 1.308905451624359, 1.2099151694491201, 1.232552563455197, 1.363113014428117, 1.4940827128713021, 1.151676363523065, 1.374363939134712, 1.4100754044624544, 1.280746501918022, 1.3844639769778342, 1.408595426868737, 1.2802202932856699, 1.3893521314704451, 1.273369642164471, 1.3859562735650828, 1.5017124154814057, 1.2560347950408344, 1.483916901328174, 1.4404708037549574, 1.1751963431030037, 1.0889573692300545, 1.227208935686357, 1.315362753723865, 1.2204819065623937, 1.4267455731204859, 1.424744946795083, 1.365189995817822, 1.1929977920600043, 1.2589831396820756, 1.2542798784161768, 1.421072910147174, 1.0997351270689377, 1.4923579411028325, 1.3842276677370726, 1.3181581462543956, 1.2614416814431983, 1.3565052123262413, 1.1355409491784036, 1.3758874822598455, 1.257687506722163, 1.4875555572006152, 1.3451999184328118, 1.356520459908044, 1.3450123547590018, 1.5139809211429491, 1.5561221360379036, 1.211052096293054, 1.444046654057944, 1.3833435107345997, 1.3969590481242005, 1.3969734158149516, 1.2956729164118577, 1.2499014154567896, 1.1666931003634844, 1.2654689358884044, 1.439235887143606, 1.1963664902963973, 1.2518137356774202, 1.459614479766224, 1.2586753677178069, 1.3007910809962047, 1.2770956315320907, 1.500460437233445, 1.2937662479661294, 1.2624676260588288, 1.3380644272947717, 1.28227173589289, 1.3044547457977647, 1.3961338407966069, 1.291428485415764, 1.1191010137362611, 1.1135955711504049, 1.1285170895913215, 1.3566344483934778, 1.3477683429353464, 1.3078046192600956, 1.6359761123477568, 1.457305803486543, 1.306281090111234, 1.4742516261481922, 1.3210661363931746, 1.5160420680340576, 1.1353355389926068, 1.2479371181554375, 1.2579095821279813, 1.6825900926690287, 1.2550614151560218, 1.3422663180284657, 1.3548896601085665, 1.3604269529635118, 1.3622922948353233, 1.2627256563584461, 1.2349341368779643, 1.3489318480043917, 1.3604971999736708, 1.2842780625375534, 1.4872101906453241, 1.4040474601452124, 1.5667688345977828, 1.3385623499122599, 1.2079460092642675, 1.5147486993674435, 1.307017245154118, 1.3323401327850504, 1.4080350974404074, 1.3872917100299713, 1.3899500850406872, 1.34769579783584, 1.2820540981117785, 1.199778001470181, 1.3249212051035073, 1.3616039097390364, 1.3868332599835953, 1.3609287746850374, 1.4368799526845029, 1.5263734225916563, 1.2971885498164086, 1.2211662383235677, 1.1492118147869008, 1.3477684403500765, 1.3749124883735808, 1.5635743045758723, 1.4172189563968913, 1.266748869032228, 1.30651676455268, 1.3946335498179, 1.499059956483143, 1.4232075546992862, 1.3255116588955695, 1.1872870210153856, 1.2480547537909805, 1.326209622402661, 1.3981676232946436, 1.4568504727116423, 1.3702770679466392, 1.3933094820638978, 1.278057833122176, 1.4042175694905268, 1.3405570634318906, 1.4145598237284969, 1.199519037075516, 1.4062133787201736, 1.3881096626926328, 1.2426262481398782, 1.3751634880909953, 1.2712932950028846, 1.3479900905515725, 1.3726937900939538, 1.1461087308881075, 1.3968474240894326, 1.3256295710469062, 1.4925646320687758, 1.288357958829248, 1.4106831803348325, 1.6087556209513212, 1.1956086717121521, 1.3009096339005515, 1.314099173918897, 1.278917687618622, 1.2343499886145612, 1.4792612327547259, 1.3789126561651333, 1.4657698451074266, 1.3499024901662784, 1.2417719686894315, 1.4529258699485683, 1.1723543168169193, 1.231163604827676, 1.2050676459110872, 1.2893317073648265, 1.2855204618185798, 1.2126275741974055, 1.1822791531674857, 1.264663038046852, 1.1085155955747903, 1.137183561503836, 1.2956296532348495, 1.4418473632420754, 1.3398015728530797, 1.2729071079236194, 1.1202084752283767, 1.3223337338663403, 1.2858206729556385, 1.3269330136628585, 1.2723869189771984, 1.3595547715019953, 1.2066175347117913, 1.2267605742581018, 1.3267144558082, 1.533546963623902, 1.3845251809399612, 1.3936469578790047, 1.2886037864217779, 1.3753364425453651, 1.3846447434936735, 1.5254633748821826, 1.238004318556239, 1.4254079779928859, 1.4762634437661442, 1.325740651248106, 1.3559544821904208, 1.453298715665708, 1.3071003122970826, 1.4441494022033132, 1.1549633790154876, 1.3564147734863627, 1.195165964496342, 1.5038651437388264, 1.3784046307583244, 1.2586915770456915, 1.2230152875716984, 1.420274862394436, 1.3317609391164411, 1.4488986972558535, 1.2398591952378295, 1.3423324620378394, 1.1813526752123296, 1.3378659093923835, 1.3464492105843753, 1.4548076727499955, 1.3470383697306472, 1.2476427136366317, 1.2474813296820977, 1.5338488836441255, 1.2663530869513657, 1.386670725452312, 1.2759314929601866, 1.323668967709604, 1.2971686931350193, 1.2970389738565884, 1.6707191756201232, 1.4425501824834017, 1.3672011522103786, 1.215406746445825, 1.4643416067147743, 1.3676343503286728, 1.4032327752913425, 1.461075841485184, 1.2070059088205123, 1.274635092287013, 1.0857705083569018, 1.2956823873455319, 1.4968875242532869, 1.4142321105627655, 1.3801031506624202, 1.2825854564781032, 1.1798932388162184, 1.1848319203036752, 1.3040401695826462, 1.2300645245558093, 1.2788315197267373, 1.3932270996737697, 1.2479309319850274, 1.3168611539008201, 1.2838636289252776, 1.440867437589887, 1.3303929905967913, 1.2332906214469856, 1.297290472344988, 1.2992439090689427, 1.1771586913195369, 1.4561105542220236, 1.262042195179262, 1.4469624779888377, 1.3105669443250714, 1.3984393682343137, 1.2415638434501364, 1.21876300696863, 1.3327050972504422, 1.1439415025226718, 1.259578934259238, 1.2817406156869242, 1.2329954601178956, 1.2060858506085474, 1.2959186489540733, 1.3921304270281503, 1.3166102787048302, 1.4136273950354143, 1.6673224104261155, 1.2450675302220495, 1.3812670359293389, 1.448693380726623, 1.0917982619301478, 1.300446146028338, 1.1095401305499877, 1.3865726880224138, 1.4193521205878554, 1.5559920276131385, 1.431591987140155, 1.2638312702866057, 1.2668492452890958, 1.1710954001705423, 1.2700269696561148, 1.442208933761225, 1.2833295626951569, 1.3790038162532707, 1.4021320320793285, 1.2590014938704448, 1.1372005900783686, 1.4480265278392719, 1.3650758896085526, 1.4001532392574378, 1.3176548326635964, 1.1957803965476286, 1.1774021820151583, 1.558287496577235, 1.3155669719511898, 1.3572739742130515, 1.559808683143708, 1.218762609763729, 1.295363105176026, 1.3113973780723145, 1.4902396256490642, 1.2429686559502096, 1.3002056196204022, 1.3818890703090583, 1.3989917623561856, 1.2431876224316136, 1.439290889153221, 1.3735000665720136, 1.20655825646708, 1.2299519951533064, 1.2676829731110275, 1.449632725410182, 1.302162191921788, 1.371072588664129, 1.4907247312896674, 1.2697282072639475, 1.2681763773441457, 1.1707592335186, 1.3373401374642713, 1.5403216238858972, 1.3903200228369845, 1.356197238171532, 1.2921997406738739, 1.4137661631437002, 1.395373436356107, 1.301199173778441, 1.2129610162746494, 1.2133514128731768, 1.2400235869132006, 1.4869675903063069, 1.3597637086516476, 1.4310497800232056, 1.2317505756019356, 1.087948911801266, 1.2220281243157738, 1.3692565745352936, 1.4616978890948058, 1.3753902249919094, 1.4288783365836608, 1.2276740367056826, 1.3539521692817107, 1.2560184197882023, 1.2012016920997481, 1.1989060818373651, 1.4879768585635498, 1.4009517952462485, 1.341249233380924, 1.2907137060341076, 1.3953584524279987, 1.2539435940746606, 1.4867412000598546, 1.3478829785731556, 1.385617012544867, 1.2836158862152696, 1.3069944303888987, 1.4630512398437845, 1.264991490247019, 1.535189848342852, 1.3764454759054292, 1.3613086024792458, 1.1945168446421623, 1.4749195344284243, 1.3314208687189144, 1.0973435522892308, 1.2648761067636254, 1.4899981768329016, 1.3494143209333505, 1.115864172386228, 1.3049253463683241, 1.415908677191919, 1.538957556766784, 1.365153954306611, 1.4554509092570829, 1.4269769695064747, 1.3225463036427112, 1.3193771054692867, 1.3417542792508192, 1.4611574462333734, 1.374430523921834, 1.311332909581378, 1.350963974576655, 1.2666059220606791, 1.3761588870271586, 1.2705309589567362, 1.3228256476448141, 1.329998001165356, 1.2136697792540254, 1.353909437372504, 1.1735819577741657, 1.4908395887680075, 1.47656931054323, 1.4455358691564195, 1.3680650966654324, 1.1049232762115997, 1.4258921705905585, 1.2534608098824367, 1.2866229382685939, 1.356024993409465, 1.4223349782398327, 1.369183557747716, 1.2849826157353832, 1.4379651326679275, 1.3350821470461987, 1.3111566804332073, 1.2365022451053744, 1.376422030113385, 1.3396058464456755, 1.2859792883761194, 1.3524104874223344, 1.3719034983150302, 1.3319246679512906, 1.3771240820177661, 1.4058941550024138, 1.3682140417390414, 1.3382417690427888, 1.3952892274393338, 1.3928217572985546, 1.3915314550702198, 1.3215907987271305, 1.3583439193869615, 1.3361965121994397, 1.2969858579912992, 1.388650490859692, 1.3454993894320488, 1.419378130063516, 1.3278003115229857, 1.3220692973412926, 1.3122015113975523, 1.3772773525287318, 1.2198635999585532, 1.0751362815555812, 1.1856134635944233, 1.352843551330985, 1.3251019478704507, 1.467209597071858, 1.4455769098031643, 1.2855920578236328, 1.282681956828004, 1.255086583183339, 1.1965530970930889, 1.5187631795352439, 1.323908403972325, 1.100634501674846, 1.4943887176513302, 1.486002627791658, 1.34864566583162, 1.3496741627651803, 1.2659550023542763, 1.1646273852182876, 1.3698004220929831, 1.3588930237688945, 1.3575276456742587, 1.3645272306507288, 1.3165026689487183, 1.5809660949548523, 1.4706930559899425, 1.2709028415952321, 1.3072109514382309, 1.284917325830719, 1.0918107970174469, 1.186909880154932, 1.3326897852637725, 1.34080569834687, 1.3022101355079942, 1.3726556402041834, 1.3798368600921735, 1.4462166970721855, 1.4221188123859358, 1.4861927557911443, 1.3450062990254636, 1.0646651432892298, 1.562255183894295, 1.3785470837650857, 1.3423658635269982, 1.2640415294557104, 1.46703795605077, 1.3859067419402031, 1.4526169145052534, 1.3355540481956876, 1.4814975172780869, 1.370126016804159, 1.3151260411332428, 1.4189226997702469, 1.4458754916684249, 1.2906617603390447, 1.2655781927093053, 1.3369456052452806, 1.3279615725504987, 1.2656684291434765, 1.2969808480690768, 1.2688646308901441, 1.3879725617033798, 1.3738115257022137, 1.4280933793302157, 1.3320508854177557, 1.3419255751319126, 1.1923971346010416, 1.3033097490982684, 1.2805672374765136, 1.1586167796562994, 1.2471921922384617, 1.2016885292119173, 1.410867950362113, 1.3142146648844553, 1.2216736610315297, 1.357151256933931, 1.4901792820892479, 1.2184335268733333, 1.4410826036249875, 1.441838199960599, 1.3204829499029547, 1.3590135305873132, 1.3373745755005717, 1.266675685480614, 1.2694231886995433, 1.2785280288463305, 1.359523797266459, 1.3579225913775717, 1.2831757788249027, 1.2888976290818663, 1.3878936981277212, 1.3479084340005514, 1.4941294962058873, 1.3849129684982748, 1.4800736332419613, 1.3921773200833638, 1.4153710985513543, 1.4269543785523138, 1.6110797586052397, 1.3702358069519271, 1.16511846914036, 1.3588809063263165, 1.3962290161663964, 1.2598369880638067, 1.1968392914092145, 1.2630530993426459, 1.3761413241004288, 1.5571249589919187, 1.2650293703658064, 1.256957401502539, 1.26189703239663, 1.1739740616704684, 1.3486393548102915, 1.556004128371493, 1.2932271417171017, 1.193273132762946, 1.5085119630977053, 1.0504821275396414, 1.1904514658973082, 1.3141916979938721, 1.3260809476288407, 1.3986111590301633, 1.3000178532416025, 1.4101156116181786, 1.4209927174470105, 1.3067379913540418, 1.2688127133271068, 1.314386200522308, 1.289799946656828, 1.0612233048557984, 1.4416347470065036, 1.5126520268608843, 1.3566030982313089, 1.267398057943246, 1.2351258240525391, 1.3557522492874456, 1.1387337696873046, 1.3440108466288698, 1.334761616123046, 1.2462943563979316, 1.0689805329510427, 1.3815673069237946, 1.5063339187213143, 1.388303858180667, 1.3464782430699223, 1.43109796841925, 1.4767878169379032, 1.4111049228974897, 1.4001727570848654, 1.2053342714882977, 1.2709848987689787, 1.2910090189184933, 1.1057828295380474, 1.2057871808177543, 1.283852802358535, 1.4625345445386195, 1.2331326608496116, 1.403393094108852, 1.414992889163075, 1.4876455210384865, 1.3312838288523878, 1.426765492409081, 1.432650040460012, 1.2605625830303548, 1.656055441224539, 1.4583406348932102, 1.3527864459134287, 1.4078218422154807, 1.287969669789947, 1.2882797386090423, 1.3413789589600678, 1.4220765676556266, 1.5745548412832602, 1.3464249421765135, 1.4373706434529294, 1.3469434233247783, 1.2193440965358067, 1.429962993610366, 1.5224704628935948, 1.2690373068826457, 1.3600473667408912, 1.262588252761385, 1.4547274718456549, 1.2073028421536316, 1.3716037375213088, 1.471624784872606, 1.4444289562107226, 1.3999336052896536, 1.4517477896310294, 1.4759652278751993, 1.2904762854492102, 1.3317156814748976, 1.354856732062505, 1.4929622597055876, 1.3269781734455277, 1.3246631085271503, 1.4439447233008755, 1.3329236801167443, 1.3524103727628431, 1.1581410856099403, 1.3908878274567835, 1.2319670734322035, 1.4165125484809278, 1.3740444797285394, 1.2755703751781093, 1.2752199520395697, 1.3274041800800012, 1.3890970274426417, 1.507535769764851, 1.3982838534634467, 1.1738886884042363, 1.2976386282667598, 1.356815114110283, 1.5432974687702865, 1.3568536068272876, 1.3309815490642203, 1.1908271880667933, 1.4827952451369526]\n"
     ]
    }
   ],
   "source": [
    "print((solver.loss_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch/layer normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0 1]\n",
      "[{'mode': 'train'}, {'mode': 'train'}, {'mode': 'train'}, {'mode': 'train'}]\n"
     ]
    }
   ],
   "source": [
    "jh = [20, 30]\n",
    "print(len(jh))\n",
    "print(np.arange(2))\n",
    "\n",
    "jh_test =FullyConnectedNet\n",
    "jh_test.num_layers =5\n",
    "jh_test.normalization = 'batchnorm'\n",
    "jh_test.bn_params = []\n",
    "if jh_test.normalization=='batchnorm':\n",
    "    jh_test.bn_params = [{'mode': 'train'} for i in range(jh_test.num_layers - 1)]\n",
    "\n",
    "print(jh_test.bn_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.3004790897684924\n",
      "W1 relative error: 7.70e-08\n",
      "W2 relative error: 1.71e-05\n",
      "W3 relative error: 2.95e-07\n",
      "b1 relative error: 4.66e-09\n",
      "b2 relative error: 2.09e-09\n",
      "b3 relative error: 6.60e-11\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  7.052114776533016\n",
      "W1 relative error: 6.86e-09\n",
      "W2 relative error: 3.52e-08\n",
      "W3 relative error: 7.13e-09\n",
      "b1 relative error: 1.17e-08\n",
      "b2 relative error: 1.72e-09\n",
      "b3 relative error: 2.87e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the learning rate and initialization scale to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 2.363364\n",
      "(Epoch 0 / 20) train acc: 0.020000; val_acc: 0.105000\n",
      "(Epoch 1 / 20) train acc: 0.020000; val_acc: 0.106000\n",
      "(Epoch 2 / 20) train acc: 0.020000; val_acc: 0.110000\n",
      "(Epoch 3 / 20) train acc: 0.020000; val_acc: 0.110000\n",
      "(Epoch 4 / 20) train acc: 0.040000; val_acc: 0.109000\n",
      "(Epoch 5 / 20) train acc: 0.040000; val_acc: 0.111000\n",
      "(Iteration 11 / 40) loss: 2.270022\n",
      "(Epoch 6 / 20) train acc: 0.040000; val_acc: 0.111000\n",
      "(Epoch 7 / 20) train acc: 0.060000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.060000; val_acc: 0.111000\n",
      "(Epoch 9 / 20) train acc: 0.040000; val_acc: 0.110000\n",
      "(Epoch 10 / 20) train acc: 0.040000; val_acc: 0.109000\n",
      "(Iteration 21 / 40) loss: 2.309562\n",
      "(Epoch 11 / 20) train acc: 0.060000; val_acc: 0.110000\n",
      "(Epoch 12 / 20) train acc: 0.060000; val_acc: 0.110000\n",
      "(Epoch 13 / 20) train acc: 0.060000; val_acc: 0.110000\n",
      "(Epoch 14 / 20) train acc: 0.060000; val_acc: 0.110000\n",
      "(Epoch 15 / 20) train acc: 0.060000; val_acc: 0.113000\n",
      "(Iteration 31 / 40) loss: 2.285026\n",
      "(Epoch 16 / 20) train acc: 0.060000; val_acc: 0.117000\n",
      "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.113000\n",
      "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.118000\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.118000\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.120000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3bc884d0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "#learning_rate = 2e-3\n",
    "#weight_scale = 1e-5\n",
    "weight_scale = np.logspace(-3,-1, 6)\n",
    "learning_rate = np.logspace(-5,-1, 6)\n",
    "\n",
    "for ws in weight_scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.302583\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.302581\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302580\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.200000; val_acc: 0.101000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.302565\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.302549\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.302557\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 1 / 20) train acc: 0.180000; val_acc: 0.102000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Iteration 11 / 40) loss: 2.302555\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.302424\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302397\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.302088\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.301873\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.301524\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.119000\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.119000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.298059\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.296773\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.299254\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.078000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.078000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.297710\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.119000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.290909\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.256074\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.078000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.080000\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.090000\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.080000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.080000\n",
      "(Iteration 11 / 40) loss: 2.302584\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.090000\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.080000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.302580\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.302574\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.119000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.302563\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.302556\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302561\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.302519\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.302414\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302335\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.115000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.114000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.102000\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 5 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Iteration 11 / 40) loss: 2.302305\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.301761\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302610\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.140000; val_acc: 0.082000\n",
      "(Iteration 1 / 40) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.107000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.300544\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.295954\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.293825\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 11 / 40) loss: 2.292341\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 21 / 40) loss: 2.259527\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 31 / 40) loss: 2.249779\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Iteration 1 / 40) loss: 2.302565\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.123000\n",
      "(Epoch 1 / 20) train acc: 0.180000; val_acc: 0.123000\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.120000\n",
      "(Epoch 3 / 20) train acc: 0.180000; val_acc: 0.119000\n",
      "(Epoch 4 / 20) train acc: 0.180000; val_acc: 0.119000\n",
      "(Epoch 5 / 20) train acc: 0.180000; val_acc: 0.121000\n",
      "(Iteration 11 / 40) loss: 2.302600\n",
      "(Epoch 6 / 20) train acc: 0.180000; val_acc: 0.123000\n",
      "(Epoch 7 / 20) train acc: 0.180000; val_acc: 0.123000\n",
      "(Epoch 8 / 20) train acc: 0.180000; val_acc: 0.122000\n",
      "(Epoch 9 / 20) train acc: 0.180000; val_acc: 0.122000\n",
      "(Epoch 10 / 20) train acc: 0.180000; val_acc: 0.120000\n",
      "(Iteration 21 / 40) loss: 2.302614\n",
      "(Epoch 11 / 20) train acc: 0.180000; val_acc: 0.117000\n",
      "(Epoch 12 / 20) train acc: 0.180000; val_acc: 0.120000\n",
      "(Epoch 13 / 20) train acc: 0.180000; val_acc: 0.119000\n",
      "(Epoch 14 / 20) train acc: 0.180000; val_acc: 0.119000\n",
      "(Epoch 15 / 20) train acc: 0.180000; val_acc: 0.118000\n",
      "(Iteration 31 / 40) loss: 2.302604\n",
      "(Epoch 16 / 20) train acc: 0.180000; val_acc: 0.117000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.117000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.117000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.118000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.115000\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.089000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 2 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.084000\n",
      "(Epoch 4 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 5 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Iteration 11 / 40) loss: 2.302558\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.083000\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.082000\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.086000\n",
      "(Epoch 9 / 20) train acc: 0.140000; val_acc: 0.082000\n",
      "(Epoch 10 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Iteration 21 / 40) loss: 2.302566\n",
      "(Epoch 11 / 20) train acc: 0.120000; val_acc: 0.086000\n",
      "(Epoch 12 / 20) train acc: 0.120000; val_acc: 0.087000\n",
      "(Epoch 13 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 14 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.084000\n",
      "(Iteration 31 / 40) loss: 2.302576\n",
      "(Epoch 16 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.084000\n",
      "(Epoch 18 / 20) train acc: 0.140000; val_acc: 0.083000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.084000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.084000\n",
      "(Iteration 1 / 40) loss: 2.302582\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.126000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.112000\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.111000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.102000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.110000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.104000\n",
      "(Iteration 11 / 40) loss: 2.302485\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.113000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.114000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.302420\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302363\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.113000\n",
      "(Iteration 1 / 40) loss: 2.302571\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.109000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.111000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.081000\n",
      "(Iteration 11 / 40) loss: 2.301971\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.301109\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.302021\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302601\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.300334\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.296239\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.289600\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.302608\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.110000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.282643\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 2.273458\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 2.273807\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 2.301820\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Iteration 11 / 40) loss: 2.304279\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 8 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Iteration 21 / 40) loss: 2.304330\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Iteration 31 / 40) loss: 2.306213\n",
      "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.073000\n",
      "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.074000\n",
      "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.074000\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.074000\n",
      "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.074000\n",
      "(Iteration 1 / 40) loss: 2.301014\n",
      "(Epoch 0 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 2 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 3 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 4 / 20) train acc: 0.200000; val_acc: 0.099000\n",
      "(Epoch 5 / 20) train acc: 0.200000; val_acc: 0.098000\n",
      "(Iteration 11 / 40) loss: 2.299525\n",
      "(Epoch 6 / 20) train acc: 0.200000; val_acc: 0.098000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 20) train acc: 0.200000; val_acc: 0.099000\n",
      "(Epoch 8 / 20) train acc: 0.200000; val_acc: 0.099000\n",
      "(Epoch 9 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 10 / 20) train acc: 0.200000; val_acc: 0.099000\n",
      "(Iteration 21 / 40) loss: 2.300361\n",
      "(Epoch 11 / 20) train acc: 0.200000; val_acc: 0.099000\n",
      "(Epoch 12 / 20) train acc: 0.200000; val_acc: 0.101000\n",
      "(Epoch 13 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 14 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 15 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Iteration 31 / 40) loss: 2.298442\n",
      "(Epoch 16 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 17 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 18 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 19 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Epoch 20 / 20) train acc: 0.200000; val_acc: 0.100000\n",
      "(Iteration 1 / 40) loss: 2.303152\n",
      "(Epoch 0 / 20) train acc: 0.040000; val_acc: 0.099000\n",
      "(Epoch 1 / 20) train acc: 0.040000; val_acc: 0.100000\n",
      "(Epoch 2 / 20) train acc: 0.040000; val_acc: 0.099000\n",
      "(Epoch 3 / 20) train acc: 0.040000; val_acc: 0.097000\n",
      "(Epoch 4 / 20) train acc: 0.040000; val_acc: 0.097000\n",
      "(Epoch 5 / 20) train acc: 0.040000; val_acc: 0.096000\n",
      "(Iteration 11 / 40) loss: 2.302341\n",
      "(Epoch 6 / 20) train acc: 0.040000; val_acc: 0.093000\n",
      "(Epoch 7 / 20) train acc: 0.040000; val_acc: 0.094000\n",
      "(Epoch 8 / 20) train acc: 0.040000; val_acc: 0.094000\n",
      "(Epoch 9 / 20) train acc: 0.040000; val_acc: 0.095000\n",
      "(Epoch 10 / 20) train acc: 0.040000; val_acc: 0.095000\n",
      "(Iteration 21 / 40) loss: 2.303247\n",
      "(Epoch 11 / 20) train acc: 0.040000; val_acc: 0.091000\n",
      "(Epoch 12 / 20) train acc: 0.040000; val_acc: 0.088000\n",
      "(Epoch 13 / 20) train acc: 0.040000; val_acc: 0.089000\n",
      "(Epoch 14 / 20) train acc: 0.040000; val_acc: 0.086000\n",
      "(Epoch 15 / 20) train acc: 0.040000; val_acc: 0.086000\n",
      "(Iteration 31 / 40) loss: 2.302905\n",
      "(Epoch 16 / 20) train acc: 0.060000; val_acc: 0.086000\n",
      "(Epoch 17 / 20) train acc: 0.060000; val_acc: 0.087000\n",
      "(Epoch 18 / 20) train acc: 0.060000; val_acc: 0.083000\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.084000\n",
      "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.083000\n",
      "(Iteration 1 / 40) loss: 2.304366\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.092000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.095000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.097000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.094000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.096000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.098000\n",
      "(Iteration 11 / 40) loss: 2.302223\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.098000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.096000\n",
      "(Epoch 8 / 20) train acc: 0.080000; val_acc: 0.097000\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.098000\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.101000\n",
      "(Iteration 21 / 40) loss: 2.303461\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.098000\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.098000\n",
      "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.100000\n",
      "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.100000\n",
      "(Epoch 15 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 31 / 40) loss: 2.298494\n",
      "(Epoch 16 / 20) train acc: 0.140000; val_acc: 0.097000\n",
      "(Epoch 17 / 20) train acc: 0.140000; val_acc: 0.094000\n",
      "(Epoch 18 / 20) train acc: 0.140000; val_acc: 0.092000\n",
      "(Epoch 19 / 20) train acc: 0.140000; val_acc: 0.093000\n",
      "(Epoch 20 / 20) train acc: 0.140000; val_acc: 0.095000\n",
      "(Iteration 1 / 40) loss: 2.302436\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.096000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.086000\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.078000\n",
      "(Epoch 3 / 20) train acc: 0.220000; val_acc: 0.082000\n",
      "(Epoch 4 / 20) train acc: 0.220000; val_acc: 0.087000\n",
      "(Epoch 5 / 20) train acc: 0.240000; val_acc: 0.084000\n",
      "(Iteration 11 / 40) loss: 2.298657\n",
      "(Epoch 6 / 20) train acc: 0.200000; val_acc: 0.080000\n",
      "(Epoch 7 / 20) train acc: 0.220000; val_acc: 0.084000\n",
      "(Epoch 8 / 20) train acc: 0.260000; val_acc: 0.077000\n",
      "(Epoch 9 / 20) train acc: 0.280000; val_acc: 0.101000\n",
      "(Epoch 10 / 20) train acc: 0.260000; val_acc: 0.097000\n",
      "(Iteration 21 / 40) loss: 2.290815\n",
      "(Epoch 11 / 20) train acc: 0.220000; val_acc: 0.085000\n",
      "(Epoch 12 / 20) train acc: 0.220000; val_acc: 0.082000\n",
      "(Epoch 13 / 20) train acc: 0.220000; val_acc: 0.082000\n",
      "(Epoch 14 / 20) train acc: 0.220000; val_acc: 0.082000\n",
      "(Epoch 15 / 20) train acc: 0.240000; val_acc: 0.080000\n",
      "(Iteration 31 / 40) loss: 2.290964\n",
      "(Epoch 16 / 20) train acc: 0.260000; val_acc: 0.100000\n",
      "(Epoch 17 / 20) train acc: 0.320000; val_acc: 0.094000\n",
      "(Epoch 18 / 20) train acc: 0.320000; val_acc: 0.117000\n",
      "(Epoch 19 / 20) train acc: 0.320000; val_acc: 0.125000\n",
      "(Epoch 20 / 20) train acc: 0.340000; val_acc: 0.131000\n",
      "(Iteration 1 / 40) loss: 2.303022\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.113000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Iteration 11 / 40) loss: 2.195836\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.220000; val_acc: 0.094000\n",
      "(Epoch 8 / 20) train acc: 0.300000; val_acc: 0.104000\n",
      "(Epoch 9 / 20) train acc: 0.360000; val_acc: 0.126000\n",
      "(Epoch 10 / 20) train acc: 0.220000; val_acc: 0.109000\n",
      "(Iteration 21 / 40) loss: 2.102048\n",
      "(Epoch 11 / 20) train acc: 0.260000; val_acc: 0.094000\n",
      "(Epoch 12 / 20) train acc: 0.060000; val_acc: 0.066000\n",
      "(Epoch 13 / 20) train acc: 0.220000; val_acc: 0.115000\n",
      "(Epoch 14 / 20) train acc: 0.260000; val_acc: 0.163000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.113000\n",
      "(Iteration 31 / 40) loss: 3.692392\n",
      "(Epoch 16 / 20) train acc: 0.200000; val_acc: 0.126000\n",
      "(Epoch 17 / 20) train acc: 0.140000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.240000; val_acc: 0.143000\n",
      "(Epoch 19 / 20) train acc: 0.280000; val_acc: 0.152000\n",
      "(Epoch 20 / 20) train acc: 0.280000; val_acc: 0.168000\n",
      "(Iteration 1 / 40) loss: 2.444123\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 5 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Iteration 11 / 40) loss: 2.636200\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Iteration 21 / 40) loss: 2.384444\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 31 / 40) loss: 2.573785\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.106000\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.104000\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.105000\n",
      "(Iteration 1 / 40) loss: 2.411105\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.097000\n",
      "(Epoch 2 / 20) train acc: 0.140000; val_acc: 0.098000\n",
      "(Epoch 3 / 20) train acc: 0.140000; val_acc: 0.098000\n",
      "(Epoch 4 / 20) train acc: 0.140000; val_acc: 0.097000\n",
      "(Epoch 5 / 20) train acc: 0.140000; val_acc: 0.097000\n",
      "(Iteration 11 / 40) loss: 2.962326\n",
      "(Epoch 6 / 20) train acc: 0.140000; val_acc: 0.097000\n",
      "(Epoch 7 / 20) train acc: 0.140000; val_acc: 0.100000\n",
      "(Epoch 8 / 20) train acc: 0.140000; val_acc: 0.098000\n",
      "(Epoch 9 / 20) train acc: 0.140000; val_acc: 0.101000\n",
      "(Epoch 10 / 20) train acc: 0.140000; val_acc: 0.099000\n",
      "(Iteration 21 / 40) loss: 2.698407\n",
      "(Epoch 11 / 20) train acc: 0.140000; val_acc: 0.100000\n",
      "(Epoch 12 / 20) train acc: 0.140000; val_acc: 0.099000\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.101000\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.099000\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.101000\n",
      "(Iteration 31 / 40) loss: 2.241747\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.106000\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.108000\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.108000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.107000\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.106000\n",
      "(Iteration 1 / 40) loss: 2.608486\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.083000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.084000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.086000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.091000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.090000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.091000\n",
      "(Iteration 11 / 40) loss: 2.608094\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.091000\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.092000\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.093000\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.094000\n",
      "(Epoch 10 / 20) train acc: 0.140000; val_acc: 0.098000\n",
      "(Iteration 21 / 40) loss: 2.279823\n",
      "(Epoch 11 / 20) train acc: 0.140000; val_acc: 0.099000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.099000\n",
      "(Epoch 13 / 20) train acc: 0.200000; val_acc: 0.107000\n",
      "(Epoch 14 / 20) train acc: 0.200000; val_acc: 0.109000\n",
      "(Epoch 15 / 20) train acc: 0.200000; val_acc: 0.109000\n",
      "(Iteration 31 / 40) loss: 2.271363\n",
      "(Epoch 16 / 20) train acc: 0.220000; val_acc: 0.113000\n",
      "(Epoch 17 / 20) train acc: 0.240000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 0.260000; val_acc: 0.110000\n",
      "(Epoch 19 / 20) train acc: 0.280000; val_acc: 0.111000\n",
      "(Epoch 20 / 20) train acc: 0.320000; val_acc: 0.110000\n",
      "(Iteration 1 / 40) loss: 2.469563\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.096000\n",
      "(Epoch 1 / 20) train acc: 0.200000; val_acc: 0.095000\n",
      "(Epoch 2 / 20) train acc: 0.280000; val_acc: 0.121000\n",
      "(Epoch 3 / 20) train acc: 0.300000; val_acc: 0.120000\n",
      "(Epoch 4 / 20) train acc: 0.400000; val_acc: 0.128000\n",
      "(Epoch 5 / 20) train acc: 0.440000; val_acc: 0.116000\n",
      "(Iteration 11 / 40) loss: 1.988066\n",
      "(Epoch 6 / 20) train acc: 0.460000; val_acc: 0.109000\n",
      "(Epoch 7 / 20) train acc: 0.460000; val_acc: 0.115000\n",
      "(Epoch 8 / 20) train acc: 0.600000; val_acc: 0.132000\n",
      "(Epoch 9 / 20) train acc: 0.660000; val_acc: 0.140000\n",
      "(Epoch 10 / 20) train acc: 0.640000; val_acc: 0.156000\n",
      "(Iteration 21 / 40) loss: 1.553241\n",
      "(Epoch 11 / 20) train acc: 0.740000; val_acc: 0.144000\n",
      "(Epoch 12 / 20) train acc: 0.740000; val_acc: 0.143000\n",
      "(Epoch 13 / 20) train acc: 0.740000; val_acc: 0.157000\n",
      "(Epoch 14 / 20) train acc: 0.780000; val_acc: 0.153000\n",
      "(Epoch 15 / 20) train acc: 0.800000; val_acc: 0.153000\n",
      "(Iteration 31 / 40) loss: 1.080230\n",
      "(Epoch 16 / 20) train acc: 0.800000; val_acc: 0.154000\n",
      "(Epoch 17 / 20) train acc: 0.820000; val_acc: 0.150000\n",
      "(Epoch 18 / 20) train acc: 0.840000; val_acc: 0.147000\n",
      "(Epoch 19 / 20) train acc: 0.860000; val_acc: 0.147000\n",
      "(Epoch 20 / 20) train acc: 0.860000; val_acc: 0.144000\n",
      "(Iteration 1 / 40) loss: 2.673081\n",
      "(Epoch 0 / 20) train acc: 0.240000; val_acc: 0.137000\n",
      "(Epoch 1 / 20) train acc: 0.280000; val_acc: 0.142000\n",
      "(Epoch 2 / 20) train acc: 0.440000; val_acc: 0.117000\n",
      "(Epoch 3 / 20) train acc: 0.580000; val_acc: 0.121000\n",
      "(Epoch 4 / 20) train acc: 0.760000; val_acc: 0.128000\n",
      "(Epoch 5 / 20) train acc: 0.780000; val_acc: 0.133000\n",
      "(Iteration 11 / 40) loss: 0.699575\n",
      "(Epoch 6 / 20) train acc: 0.780000; val_acc: 0.171000\n",
      "(Epoch 7 / 20) train acc: 0.920000; val_acc: 0.128000\n",
      "(Epoch 8 / 20) train acc: 0.900000; val_acc: 0.157000\n",
      "(Epoch 9 / 20) train acc: 0.840000; val_acc: 0.162000\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.149000\n",
      "(Iteration 21 / 40) loss: 0.397927\n",
      "(Epoch 11 / 20) train acc: 0.960000; val_acc: 0.140000\n",
      "(Epoch 12 / 20) train acc: 0.960000; val_acc: 0.157000\n",
      "(Epoch 13 / 20) train acc: 0.960000; val_acc: 0.152000\n",
      "(Epoch 14 / 20) train acc: 0.980000; val_acc: 0.141000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.161000\n",
      "(Iteration 31 / 40) loss: 0.078688\n",
      "(Epoch 16 / 20) train acc: 0.980000; val_acc: 0.153000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.155000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.159000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.159000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Iteration 1 / 40) loss: 2.548853\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.109000\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.120000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.087000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joohong/spring1718_assignment2_v2/cs231n/classifiers/fc_net.py:389: RuntimeWarning: overflow encountered in exp\n",
      "  scores_exp = np.exp(scores)\n",
      "/home/joohong/spring1718_assignment2_v2/cs231n/classifiers/fc_net.py:390: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probs = scores_exp/np.sum(scores_exp,axis=1,keepdims=True)\n",
      "/home/joohong/spring1718_assignment2_v2/cs231n/classifiers/fc_net.py:391: RuntimeWarning: divide by zero encountered in log\n",
      "  correct_log_probs = -np.log(probs[range(X.shape[0]),y])\n",
      "/home/joohong/spring1718_assignment2_v2/cs231n/classifiers/fc_net.py:424: RuntimeWarning: invalid value encountered in greater\n",
      "  self.bk_params[\"dX\" + str(layer+1) + \"_RELU\"] = (self.bk_params[\"X_tmp\"+str(layer+1)] > 0) * self.bk_params[\"dX\"+str(layer+2)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 11 / 40) loss: nan\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 8 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 21 / 40) loss: nan\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 31 / 40) loss: nan\n",
      "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 1 / 40) loss: 84.302036\n",
      "(Epoch 0 / 20) train acc: 0.060000; val_acc: 0.092000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.093000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.092000\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.096000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.097000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.095000\n",
      "(Iteration 11 / 40) loss: 86.552628\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.096000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.100000\n",
      "(Epoch 8 / 20) train acc: 0.060000; val_acc: 0.097000\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.101000\n",
      "(Iteration 21 / 40) loss: 32.774820\n",
      "(Epoch 11 / 20) train acc: 0.140000; val_acc: 0.100000\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.100000\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.101000\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.103000\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.101000\n",
      "(Iteration 31 / 40) loss: 64.804997\n",
      "(Epoch 16 / 20) train acc: 0.140000; val_acc: 0.100000\n",
      "(Epoch 17 / 20) train acc: 0.140000; val_acc: 0.104000\n",
      "(Epoch 18 / 20) train acc: 0.140000; val_acc: 0.102000\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.105000\n",
      "(Epoch 20 / 20) train acc: 0.180000; val_acc: 0.105000\n",
      "(Iteration 1 / 40) loss: 163.413040\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.126000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.126000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.121000\n",
      "(Epoch 3 / 20) train acc: 0.180000; val_acc: 0.109000\n",
      "(Epoch 4 / 20) train acc: 0.280000; val_acc: 0.107000\n",
      "(Epoch 5 / 20) train acc: 0.260000; val_acc: 0.113000\n",
      "(Iteration 11 / 40) loss: 22.105417\n",
      "(Epoch 6 / 20) train acc: 0.320000; val_acc: 0.114000\n",
      "(Epoch 7 / 20) train acc: 0.320000; val_acc: 0.120000\n",
      "(Epoch 8 / 20) train acc: 0.480000; val_acc: 0.114000\n",
      "(Epoch 9 / 20) train acc: 0.500000; val_acc: 0.116000\n",
      "(Epoch 10 / 20) train acc: 0.520000; val_acc: 0.115000\n",
      "(Iteration 21 / 40) loss: 8.894167\n",
      "(Epoch 11 / 20) train acc: 0.660000; val_acc: 0.125000\n",
      "(Epoch 12 / 20) train acc: 0.720000; val_acc: 0.123000\n",
      "(Epoch 13 / 20) train acc: 0.780000; val_acc: 0.117000\n",
      "(Epoch 14 / 20) train acc: 0.880000; val_acc: 0.120000\n",
      "(Epoch 15 / 20) train acc: 0.880000; val_acc: 0.118000\n",
      "(Iteration 31 / 40) loss: 0.311534\n",
      "(Epoch 16 / 20) train acc: 0.940000; val_acc: 0.119000\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.117000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.117000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.117000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.117000\n",
      "(Iteration 1 / 40) loss: 147.242801\n",
      "(Epoch 0 / 20) train acc: 0.180000; val_acc: 0.128000\n",
      "(Epoch 1 / 20) train acc: 0.180000; val_acc: 0.085000\n",
      "(Epoch 2 / 20) train acc: 0.300000; val_acc: 0.123000\n",
      "(Epoch 3 / 20) train acc: 0.440000; val_acc: 0.111000\n",
      "(Epoch 4 / 20) train acc: 0.560000; val_acc: 0.120000\n",
      "(Epoch 5 / 20) train acc: 0.820000; val_acc: 0.108000\n",
      "(Iteration 11 / 40) loss: 2.133443\n",
      "(Epoch 6 / 20) train acc: 0.940000; val_acc: 0.112000\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.109000\n",
      "(Epoch 8 / 20) train acc: 0.960000; val_acc: 0.109000\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.112000\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.112000\n",
      "(Iteration 21 / 40) loss: 0.007178\n",
      "(Epoch 11 / 20) train acc: 0.980000; val_acc: 0.111000\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.113000\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Iteration 31 / 40) loss: 0.000417\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.112000\n",
      "(Iteration 1 / 40) loss: 86.616025\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.082000\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 2 / 20) train acc: 0.220000; val_acc: 0.098000\n",
      "(Epoch 3 / 20) train acc: 0.380000; val_acc: 0.090000\n",
      "(Epoch 4 / 20) train acc: 0.460000; val_acc: 0.099000\n",
      "(Epoch 5 / 20) train acc: 0.620000; val_acc: 0.107000\n",
      "(Iteration 11 / 40) loss: 7.390862\n",
      "(Epoch 6 / 20) train acc: 0.720000; val_acc: 0.113000\n",
      "(Epoch 7 / 20) train acc: 0.900000; val_acc: 0.120000\n",
      "(Epoch 8 / 20) train acc: 0.920000; val_acc: 0.129000\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.120000\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.120000\n",
      "(Iteration 21 / 40) loss: 0.041851\n",
      "(Epoch 11 / 20) train acc: 0.980000; val_acc: 0.118000\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.119000\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.119000\n",
      "(Epoch 14 / 20) train acc: 0.980000; val_acc: 0.117000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.114000\n",
      "(Iteration 31 / 40) loss: 0.005141\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.115000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.118000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.118000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.117000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.117000\n",
      "(Iteration 1 / 40) loss: 198.275931\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 11 / 40) loss: nan\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 8 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 21 / 40) loss: nan\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 31 / 40) loss: nan\n",
      "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 1 / 40) loss: 117.855348\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(Epoch 1 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 2 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 4 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 11 / 40) loss: nan\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 7 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 8 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 21 / 40) loss: nan\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.087000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 31 / 40) loss: nan\n",
      "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.087000\n",
      "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.087000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f560bb67d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate_for_training_set_overfitting = %f 0.01584893192461114\n",
      "weight_scale_for_training_set_overfitting = %f 0.039810717055349734\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "#learning_rate = 2e-3\n",
    "#weight_scale = 1e-5\n",
    "weight_scale = np.logspace(-3,-1, 6)\n",
    "learning_rate = np.logspace(-5,-1, 6)\n",
    "best_train_acc = 0;\n",
    "for ws in weight_scale:\n",
    "    for lr in learning_rate:\n",
    "        model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=ws, dtype=np.float64)\n",
    "        solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': lr,\n",
    "                }\n",
    "         )\n",
    "        solver.train()\n",
    "        if best_train_acc < solver.train_acc_history[-1] :\n",
    "            best_train_acc = solver.train_acc_history[-1] \n",
    "            best_learning_rate = lr\n",
    "            best_weight_scale = ws\n",
    "            best_train_acc_history = solver.train_acc_history\n",
    "            \n",
    "plt.plot(best_train_acc_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n",
    "print('learning_rate_for_training_set_overfitting = ', best_learning_rate)\n",
    "print('weight_scale_for_training_set_overfitting = ', best_weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-f167b5108958>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-f167b5108958>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print('learning_rate_for_training_set_overfitting = ' best_learning_rate)\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 2: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5ef1f64ea18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Should see relative errors around e-8 or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_w error: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_next_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'velocity error: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_velocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'velocity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-41978332fcd2>\u001b[0m in \u001b[0;36mrel_error\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;34m\"\"\" returns relative error \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 3:\n",
    "\n",
    "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
